{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è©•ä¾¡æ‰‹æ³•ã®æ¤œè¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrV21bF8yUQL",
    "outputId": "bd9e18c0-358b-49ad-9bf7-11c7c05d947b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: ç’°å¢ƒè¨­å®šã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\"\"\"\n",
    "LA-Bench 2025: å®Ÿé¨“æ‰‹é †ç”Ÿæˆã‚¿ã‚¹ã‚¯\n",
    "Baseline Implementation for Google Colaboratory\n",
    "GitHub: https://github.com/lasa-or-jp/la-bench.git\n",
    "\"\"\"\n",
    "\n",
    "#@title 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— { display-mode: \"form\" }\n",
    "#@markdown ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ã€‚\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colabã‹ã©ã†ã‹ã®ç¢ºèª\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colaboratoryç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\")\n",
    "    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    print(\"\\nğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    !pip install -q openai pyyaml tqdm pandas\n",
    "\n",
    "    print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "\n",
    "    # GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "    REPO_URL = \"https://github.com/lasa-or-jp/la-bench.git\"\n",
    "    REPO_NAME = \"la-bench\"\n",
    "\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        print(f\"\\nğŸ“¥ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­: {REPO_URL}\")\n",
    "        !git clone -q {REPO_URL}\n",
    "        print(f\"âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†: {REPO_NAME}/\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒªã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {REPO_NAME}/\")\n",
    "        print(\"ğŸ“¥ æœ€æ–°ç‰ˆã«æ›´æ–°ä¸­...\")\n",
    "        !cd {REPO_NAME} && git pull -q\n",
    "        print(\"âœ… æ›´æ–°å®Œäº†\")\n",
    "\n",
    "    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š\n",
    "    WORK_DIR = Path(REPO_NAME)\n",
    "    os.chdir(WORK_DIR)\n",
    "    print(f\"\\nğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
    "\n",
    "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ç¢ºèª\n",
    "    print(\"\\nğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ :\")\n",
    "    !ls -la\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\")\n",
    "    if Path.cwd().name == \"notebooks\":\n",
    "        os.chdir(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PZr_6rXsyqlK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ APIã‚­ãƒ¼: ********************6tcA\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: OpenAI APIã‚­ãƒ¼ã®è¨­å®š\n",
    "#@title 2. OpenAI API Keyè¨­å®š { display-mode: \"form\" }\n",
    "#@markdown OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ã¯å®‰å…¨ã«ç®¡ç†ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "\n",
    "# APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•ã‚’é¸æŠ\n",
    "use_secrets = True  #@param {type:\"boolean\"}\n",
    "#@markdown â˜ï¸ Google Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "if IN_COLAB:\n",
    "    import getpass\n",
    "    from google.colab import userdata\n",
    "    if use_secrets:\n",
    "        try:\n",
    "            # Colab Secretsã‹ã‚‰å–å¾—\n",
    "            API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "            print(\"âœ… APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ\")\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Secretsã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "            print(\"å·¦å´ã®ãƒ‘ãƒãƒ«ã®ğŸ”‘ã‚¢ã‚¤ã‚³ãƒ³ã‹ã‚‰'OPENAI_API_KEY'ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
    "            API_KEY = None\n",
    "    else:\n",
    "        # ç›´æ¥å…¥åŠ›\n",
    "        api_key_input = getpass.getpass(\"ğŸ”‘ OpenAI API Keyã‚’å…¥åŠ›: \")\n",
    "        if api_key_input:\n",
    "            API_KEY = api_key_input\n",
    "            os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "            print(\"âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ\")\n",
    "        else:\n",
    "            API_KEY = None\n",
    "            print(\"âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ‰‹æ³•ã®ã¿ä½¿ç”¨ï¼‰\")\n",
    "else:\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
    "    API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not API_KEY:\n",
    "        API_KEY = input(\"OpenAI API Key: \")\n",
    "\n",
    "# APIã‚­ãƒ¼ã®æ¤œè¨¼\n",
    "if API_KEY:\n",
    "    print(f\"ğŸ”‘ APIã‚­ãƒ¼: {'*' * 20}{API_KEY[-4:]}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPTæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "71dPwmdnzEfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LA-Bench 2025 Baseline Implementation\n",
      "å®Ÿè¡Œç’°å¢ƒ: Local\n",
      "å®Ÿè¡Œæ™‚åˆ»: 2025-12-23 14:57:56\n",
      "OpenAIåˆ©ç”¨å¯èƒ½: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "#@title 3. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ { display-mode: \"form\" }\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Set, Any\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# OpenAI API\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"âš ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ (Colabå¯¾å¿œ)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LA-Bench 2025 Baseline Implementation\")\n",
    "print(f\"å®Ÿè¡Œç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"OpenAIåˆ©ç”¨å¯èƒ½: {OPENAI_AVAILABLE and API_KEY is not None}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0rypmgB20Yc2"
   },
   "outputs": [],
   "source": [
    "# Cell 4: ãƒ‡ãƒ¼ã‚¿æ§‹é€ \n",
    "#@title 4. ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å®šç¾© { display-mode: \"form\" }\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    id: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ReferenceEntry:\n",
    "    id: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ExampleInput:\n",
    "    instruction: str\n",
    "    mandatory_objects: Set[str] = field(default_factory=set)\n",
    "    source_protocol_steps: List[Step] = field(default_factory=list)\n",
    "    expected_final_states: Set[str] = field(default_factory=set)\n",
    "    references: List[ReferenceEntry] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class ExampleOutput:\n",
    "    procedure_steps: List[Step] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class Measurement:\n",
    "    specific_criteria: Dict[str, int] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class ExampleSample:\n",
    "    id: str\n",
    "    input: ExampleInput\n",
    "    output: ExampleOutput\n",
    "    measurement: Optional[Measurement] = None\n",
    "\n",
    "def _to_set(x):\n",
    "    return set(x) if isinstance(x, (list, set, tuple)) else set()\n",
    "\n",
    "def _to_list(x):\n",
    "    return list(x) if isinstance(x, (list, set, tuple)) else (x if isinstance(x, list) else [])\n",
    "\n",
    "def _to_steps(x) -> List[Step]:\n",
    "    steps: List[Step] = []\n",
    "    arr = _to_list(x)\n",
    "    if not arr:\n",
    "        return steps\n",
    "    if isinstance(arr[0], dict):\n",
    "        for it in arr:\n",
    "            try:\n",
    "                sid = int(it.get(\"id\", len(steps) + 1))\n",
    "            except Exception:\n",
    "                sid = len(steps) + 1\n",
    "            steps.append(Step(id=sid, text=str(it.get(\"text\", \"\")).strip()))\n",
    "    else:\n",
    "        for idx, s in enumerate(arr, start=1):\n",
    "            steps.append(Step(id=idx, text=str(s).strip()))\n",
    "    return steps\n",
    "\n",
    "def _to_references(x) -> List[ReferenceEntry]:\n",
    "    refs: List[ReferenceEntry] = []\n",
    "    arr = _to_list(x)\n",
    "    if not arr:\n",
    "        return refs\n",
    "    if isinstance(arr[0], dict):\n",
    "        for it in arr:\n",
    "            try:\n",
    "                rid = int(it.get(\"id\", len(refs) + 1))\n",
    "            except Exception:\n",
    "                rid = len(refs) + 1\n",
    "            refs.append(ReferenceEntry(id=rid, text=str(it.get(\"text\", \"\")).strip()))\n",
    "    else:\n",
    "        for idx, ref in enumerate(arr, start=1):\n",
    "            refs.append(ReferenceEntry(id=idx, text=str(ref).strip()))\n",
    "    return refs\n",
    "\n",
    "def parse_sample(obj: Dict[str, Any]) -> ExampleSample:\n",
    "    sid = obj.get(\"id\") or obj.get(\"sample_id\") or \"unknown\"\n",
    "    i = obj.get(\"input\", {})\n",
    "    o = obj.get(\"output\", {})\n",
    "    m = obj.get(\"measurement\", {})\n",
    "\n",
    "    # Measurement.specific_criteria ã‚’ dict ã«æ­£è¦åŒ–ï¼ˆlistå½¢å¼ã‚‚è¨±å®¹ï¼‰\n",
    "    sc_raw = m.get(\"specific_criteria\", {})\n",
    "    sc: Dict[str, int] = {}\n",
    "    if isinstance(sc_raw, dict):\n",
    "        for k, v in sc_raw.items():\n",
    "            try:\n",
    "                sc[str(k)] = int(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif isinstance(sc_raw, list):\n",
    "        for it in sc_raw:\n",
    "            try:\n",
    "                k = it.get(\"item\")\n",
    "                v = int(it.get(\"score\", 0))\n",
    "                if k:\n",
    "                    sc[str(k)] = v\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    sample = ExampleSample(\n",
    "        id=str(sid),\n",
    "        input=ExampleInput(\n",
    "            instruction=str(i.get(\"instruction\", \"\")).strip(),\n",
    "            mandatory_objects=_to_set(i.get(\"mandatory_objects\", [])),\n",
    "            source_protocol_steps=_to_steps(i.get(\"source_protocol_steps\", [])),\n",
    "            expected_final_states=_to_set(i.get(\"expected_final_states\", [])),\n",
    "            references=_to_references(i.get(\"references\", [])),\n",
    "        ),\n",
    "        output=ExampleOutput(\n",
    "            procedure_steps=_to_steps(o.get(\"procedure_steps\", []))\n",
    "        ),\n",
    "        measurement=Measurement(specific_criteria=sc) if sc else None\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "def load_example_jsonl(path: str):\n",
    "    samples = []\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"JSONL not found: {p}\")\n",
    "    for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSONL parse error: {e}\")\n",
    "            continue\n",
    "        samples.append(parse_sample(obj))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MXF0Ou0B6sFB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 10 samples from data/private_test/private_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: JSONLãƒ­ãƒ¼ãƒ€ãƒ¼ã®åˆ©ç”¨\n",
    "#@title 5. JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ { display-mode: \"form\" }\n",
    "#@markdown exampleã‚’ä½¿ã†ã¨ãï¼š`data/example/example.jsonl`\n",
    "#@markdown public_testã‚’ä½¿ã†ã¨ãï¼š`data/public_test/public_test.jsonl`\n",
    "\n",
    "# jsonl_path = 'data/example/example.jsonl'  #@param {type:'string'}\n",
    "# jsonl_path = 'data/public_test/public_test.jsonl'  #@param {type:'string'}\n",
    "jsonl_path = 'data/private_test/private_test.jsonl'  #@param {type:'string'}\n",
    "\n",
    "try:\n",
    "    samples = load_example_jsonl(jsonl_path)\n",
    "    print(f'âœ… Loaded {len(samples)} samples from {jsonl_path}')\n",
    "except Exception as e:\n",
    "    print(f'âŒ Load error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WZ94EpzOInJr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:58:36 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "14:59:46 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:01:43 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:03:11 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:04:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:05:19 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:06:04 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:06:43 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:07:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:08:26 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç”Ÿæˆå®Œäº†: 10 samples\n",
      "ä¾‹: private_test_1 â†’ 16 steps\n",
      "ğŸ“„ Saved JSONL: outputs/runs/generated_20251223_150826.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/generated_20251223_150826.jsonl' target='_blank'>/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/generated_20251223_150826.jsonl</a><br>"
      ],
      "text/plain": [
       "/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/generated_20251223_150826.jsonl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: å®Ÿé¨“æ‰‹é †ã®ç”Ÿæˆï¼ˆOpenAI, Pydanticæ§‹é€ åŒ–ï¼‰\n",
    "#@title 6. LLMã§ Input ã‹ã‚‰ Outputï¼ˆprocedure_stepsï¼‰ã‚’ç”Ÿæˆ { display-mode: \"form\" }\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\" #@param [\"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-08-06\", \"gpt-5-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-nano-2025-08-07\"]\n",
    "#@markdown gpt-4o-mini, gpt-4o-2024-08-06, ã‚ã‚‹ã„ã¯ãã‚Œä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚<br>\n",
    "#@markdown (Structured outputã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚) <br>\n",
    "#@markdown gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€temperature=1.0ã¨ã—ã¦ãã ã•ã„ã€‚\n",
    "TEMPERATURE = 1.0 # @param\n",
    "#@markdown `build_messages`é–¢æ•°ã«ãŠã„ã¦ã€LLMã®å…¥åŠ›ã‚’è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "class StepModel(BaseModel):\n",
    "    id: int = Field(ge=1, description=\"ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·\")\n",
    "    text: str = Field(description=\"å®Ÿé¨“æ‰‹é †ã®è©³ç´°ãªèª¬æ˜\")\n",
    "\n",
    "class GeneratedOutput(BaseModel):\n",
    "    procedure_steps: List[StepModel] = Field(\n",
    "        description=\"å®Ÿé¨“æ‰‹é †ã®ãƒªã‚¹ãƒˆ\",\n",
    "        min_items=1,\n",
    "        max_items=50\n",
    "    )\n",
    "\n",
    "def build_messages(sample: ExampleSample) -> list[dict]:\n",
    "    sys = (\n",
    "        \"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã® Input ã‚’èª­ã¿ã€\"\n",
    "        \"æ—¥æœ¬èªã§å®Ÿè¡Œå¯èƒ½ãªå®Ÿé¨“æ‰‹é †ï¼ˆprocedure_stepsï¼‰ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\"\n",
    "        \"åˆ¶ç´„: ã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯æœ€å¤§50ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã¯10æ–‡ä»¥ä¸‹ã€idã¯1ã‹ã‚‰æ˜‡é †ã€‚\"\n",
    "    )\n",
    "    user_lines = []\n",
    "    user_lines.append(f\"ã€å®Ÿé¨“æŒ‡ç¤ºã€‘\\n{sample.input.instruction}\")\n",
    "    if sample.input.mandatory_objects:\n",
    "        user_lines.append(\"\\nã€ä½¿ç”¨ã™ã‚‹ç‰©å“ã€‘\")\n",
    "        for it in sorted(sample.input.mandatory_objects):\n",
    "            user_lines.append(f\"- {it}\")\n",
    "    if sample.input.source_protocol_steps:\n",
    "        user_lines.append(\"\\nã€å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ï¼ˆå‚è€ƒï¼‰ã€‘\")\n",
    "        for st in sample.input.source_protocol_steps:\n",
    "            user_lines.append(f\"- {st.id}. {st.text}\")\n",
    "    if sample.input.expected_final_states:\n",
    "        user_lines.append(\"\\nã€æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€‘\")\n",
    "        for fs in sorted(sample.input.expected_final_states):\n",
    "            user_lines.append(f\"- {fs}\")\n",
    "    if sample.input.references:\n",
    "        user_lines.append(\"\\nã€å‚è€ƒæ–‡çŒ®ã€‘\")\n",
    "        for ref in sample.input.references:\n",
    "            user_lines.append(f\"- [{ref.id}] {ref.text}\")\n",
    "    usr = \"\\n\".join(user_lines)\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": usr},\n",
    "    ]\n",
    "\n",
    "def generate_outputs(samples: list[ExampleSample]) -> list[dict]:\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    results: list[dict] = []\n",
    "    for sm in samples:\n",
    "        msgs = build_messages(sm)\n",
    "        try:\n",
    "            completion = client.chat.completions.parse(\n",
    "                model=MODEL_NAME,\n",
    "                messages=msgs,\n",
    "                temperature=TEMPERATURE,\n",
    "                response_format=GeneratedOutput,\n",
    "            )\n",
    "            parsed: GeneratedOutput = completion.choices[0].message.parsed  # type: ignore\n",
    "            steps = [\n",
    "                Step(id=s.id, text=s.text)\n",
    "                for s in sorted(parsed.procedure_steps, key=lambda x: x.id)\n",
    "            ][:50]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç”Ÿæˆå¤±æ•—: {sm.id}: {e}\")\n",
    "            steps = []  # no fallback\n",
    "        results.append({\n",
    "            \"id\": sm.id,\n",
    "            \"procedure_steps\": [{\"id\": s.id, \"text\": s.text} for s in steps],\n",
    "        })\n",
    "    print(f\"âœ… ç”Ÿæˆå®Œäº†: {len(results)} samples\")\n",
    "    return results\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "generated_results = generate_outputs(samples)\n",
    "if generated_results:\n",
    "    print(f\"ä¾‹: {generated_results[0]['id']} â†’ {len(generated_results[0]['procedure_steps'])} steps\")\n",
    "\n",
    "# ç”Ÿæˆçµæœã‚’ JSONL ã§ä¿å­˜ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º\n",
    "ts = time.strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = Path('./outputs/runs')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "jsonl_path = out_dir / f'generated_{ts}.jsonl'\n",
    "with jsonl_path.open('w', encoding='utf-8') as f:\n",
    "    for rec in generated_results:\n",
    "        obj = {\"id\": rec[\"id\"], \"output\": {\"procedure_steps\": rec[\"procedure_steps\"]}}\n",
    "        line = json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "        f.write(line + \"\\n\")\n",
    "print(f\"ğŸ“„ Saved JSONL: {jsonl_path}\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆColab/ãƒ­ãƒ¼ã‚«ãƒ«åŒæ–¹ã«å¯¾å¿œï¼‰\n",
    "try:\n",
    "    from google.colab import files as colab_files  # type: ignore\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’å‡ºã—ã¦ã€yãªã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    from google.colab.output import eval_js\n",
    "    print(f\"Download file: {jsonl_path}\")\n",
    "    confirm = eval_js('confirm(\"ç”Ÿæˆã•ã‚ŒãŸJSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ\")')\n",
    "    if confirm:\n",
    "      colab_files.download(str(jsonl_path))\n",
    "    else:\n",
    "      print(\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "except Exception:\n",
    "    from IPython.display import FileLink, display\n",
    "    display(FileLink(str(jsonl_path.resolve())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wYtMf3bJOzcN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:08:54 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:09:33 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:10:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:11:45 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:12:33 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:13:07 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:13:26 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:13:47 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:14:27 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:14:59 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM-as-a-judge: Scored 10 samples (0-10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "general_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "specific_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "14a96165-d6d1-40e6-97f8-1cea93351415",
       "rows": [
        [
         "0",
         "private_test_1",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "1",
         "private_test_10",
         "4.0",
         "2.0",
         "6.0"
        ],
        [
         "2",
         "private_test_2",
         "4.0",
         "1.0",
         "5.0"
        ],
        [
         "3",
         "private_test_3",
         "5.0",
         "4.0",
         "9.0"
        ],
        [
         "4",
         "private_test_4",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "5",
         "private_test_5",
         "3.0",
         "5.0",
         "8.0"
        ],
        [
         "6",
         "private_test_6",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "7",
         "private_test_7",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "8",
         "private_test_8",
         "4.0",
         "3.0",
         "7.0"
        ],
        [
         "9",
         "private_test_9",
         "4.0",
         "4.0",
         "8.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>general_score</th>\n",
       "      <th>specific_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private_test_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private_test_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private_test_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private_test_3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>private_test_4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>private_test_5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>private_test_6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>private_test_7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private_test_8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>private_test_9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  general_score  specific_score  total_score\n",
       "0   private_test_1            5.0             5.0         10.0\n",
       "1  private_test_10            4.0             2.0          6.0\n",
       "2   private_test_2            4.0             1.0          5.0\n",
       "3   private_test_3            5.0             4.0          9.0\n",
       "4   private_test_4            5.0             5.0         10.0\n",
       "5   private_test_5            3.0             5.0          8.0\n",
       "6   private_test_6            5.0             5.0         10.0\n",
       "7   private_test_7            5.0             5.0         10.0\n",
       "8   private_test_8            4.0             3.0          7.0\n",
       "9   private_test_9            4.0             4.0          8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Saved: outputs/runs/eval_gpt-5-mini-2025-08-07_20251223_150826.csv\n",
      "Download file: outputs/runs/eval_gpt-5-mini-2025-08-07_20251223_150826.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_gpt-5-mini-2025-08-07_20251223_150826.csv' target='_blank'>/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_gpt-5-mini-2025-08-07_20251223_150826.csv</a><br>"
      ],
      "text/plain": [
       "/Users/kato/Dropbox/Programs/gensurv/la-bench/outputs/runs/eval_gpt-5-mini-2025-08-07_20251223_150826.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: LLM-as-a-judge è©•ä¾¡ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰\n",
    "#@title 7. LLM ã§å…±é€š5ç‚¹ + å€‹åˆ¥5ç‚¹ã‚’æ¡ç‚¹ { display-mode: \"form\" }\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"OpenAI SDK v1 ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`uv add openai` ã§è¿½åŠ ã—ã¦ãã ã•ã„ã€‚\") from e\n",
    "\n",
    "JUDGE_MODEL = \"gpt-5-mini-2025-08-07\"  #@param [\"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-08-06\", \"gpt-5-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-nano-2025-08-07\"]\n",
    "JUDGE_TEMPERATURE = 1.0\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    general_score: float = Field(ge=0, le=5)\n",
    "    specific_score: float = Field(ge=0, le=5)\n",
    "    final_score: float = Field(ge=0, le=10)\n",
    "    general_reason: str\n",
    "    specific_matches: List[str] = []\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "def build_judge_messages(sample: ExampleSample, steps: List[Step]) -> list[dict]:\n",
    "    # è©•ä¾¡åŸºæº–ï¼ˆå…±é€š5ç‚¹ + å€‹åˆ¥5ç‚¹ï¼‰\n",
    "    system = (\n",
    "        \"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…¬å¹³ãªæ¡ç‚¹è€…ã§ã™ã€‚\"\n",
    "        \"ä»¥ä¸‹ã®åŸºæº–ã«å¾“ã£ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸ Input ã«å¯¾ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ï¼ˆOutputï¼‰ã€‘ã‚’è©•ä¾¡ã—ã€\"\n",
    "        \"general_score(0-5) ã¨ specific_score(0-5) ã¨ final_score(0-10) ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\"\n",
    "        \"\\n\\n[å…±é€šæ¡ç‚¹åŸºæº– 5ç‚¹æº€ç‚¹]\\n\"\n",
    "        \"åŠ ç‚¹(+1ãšã¤): 1) å®Ÿé¨“æŒ‡ç¤ºã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åæ˜ , 2) ä½¿ç”¨ã™ã‚‹ç‰©å“ã®åæ˜ , 3) å…ƒæ‰‹é †ã®è«–ç†åæ˜ , 4) æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã®é”æˆ, 5) é©åˆ‡ãªè£œå®Œã€‚\\n\"\n",
    "        \"æ¸›ç‚¹: ä¸è‡ªç„¶ãªæ—¥æœ¬èª/ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³, è¨ˆç®—ãƒŸã‚¹, æ‰‹é †çŸ›ç›¾ã€‚\\n\"\n",
    "        \"ä¸Šé™: å…¥åŠ›æ‰‹é †ã®ä¸¸å†™ã—ç­‰ã®éåº¦ã®å®‰å…¨æ€§ãŒè¦‹ã‚‰ã‚Œã‚‹å ´åˆã€general_score ã¯æœ€å¤§2ç‚¹ã«åˆ¶é™ã€‚\\n\\n\"\n",
    "        \"[å€‹åˆ¥æ¡ç‚¹åŸºæº– 5ç‚¹æº€ç‚¹]\\n\"\n",
    "        \"ä¸ãˆã‚‰ã‚ŒãŸ specific_criteria ã®å„ item ãŒæ‰‹é †ã«å«ã¾ã‚Œã‚‹/æº€ãŸã™ãªã‚‰ã€ãã® score ã‚’åŠ ç‚¹ï¼ˆåˆè¨ˆ5ç‚¹ã§ä¸Šé™ï¼‰ã€‚\"\n",
    "    )\n",
    "\n",
    "    parts = []\n",
    "    parts.append(f\"ã€å®Ÿé¨“æŒ‡ç¤ºã€‘\\n{sample.input.instruction}\")\n",
    "    if sample.input.mandatory_objects:\n",
    "        parts.append(\"\\nã€ä½¿ç”¨ã™ã‚‹ç‰©å“ã€‘\")\n",
    "        for it in sorted(sample.input.mandatory_objects):\n",
    "            parts.append(f\"- {it}\")\n",
    "    if sample.input.source_protocol_steps:\n",
    "        parts.append(\"\\nã€å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ï¼ˆå‚è€ƒï¼‰ã€‘\")\n",
    "        for st in sample.input.source_protocol_steps:\n",
    "            parts.append(f\"- {st.id}. {st.text}\")\n",
    "    if sample.input.expected_final_states:\n",
    "        parts.append(\"\\nã€æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€‘\")\n",
    "        for fs in sorted(sample.input.expected_final_states):\n",
    "            parts.append(f\"- {fs}\")\n",
    "    if sample.input.references:\n",
    "        parts.append(\"\\nã€å‚è€ƒæ–‡çŒ®ã€‘\")\n",
    "        for ref in sample.input.references:\n",
    "            parts.append(f\"- [{ref.id}] {ref.text}\")\n",
    "\n",
    "    parts.append(\"\\nã€ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ï¼ˆOutputï¼‰ã€‘\")\n",
    "    for s in steps:\n",
    "        parts.append(f\"- {s.id}. {s.text}\")\n",
    "\n",
    "    parts.append(\"\\nã€specific_criteriaã€‘\")\n",
    "    if sample.measurement and sample.measurement.specific_criteria:\n",
    "        for item, sc in sample.measurement.specific_criteria.items():\n",
    "            parts.append(f\"- ({int(sc)}ç‚¹) {item}\")\n",
    "    else:\n",
    "        parts.append(\"- ãªã—\")\n",
    "\n",
    "    user = \"\\n\".join(parts)\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "def judge_with_llm(samples: List[ExampleSample], generated: list[dict]) -> pd.DataFrame:\n",
    "    client = OpenAI(api_key=API_KEY) if 'API_KEY' in globals() and API_KEY else OpenAI()\n",
    "    proc_map = {g['id']: [Step(id=it['id'], text=it['text']) for it in g['procedure_steps']] for g in generated}\n",
    "    rows = []\n",
    "    quota_exhausted = False\n",
    "    def _is_insufficient_quota(err: Exception) -> bool:\n",
    "        s = str(err)\n",
    "        return 'insufficient_quota' in s or 'You exceeded your current quota' in s\n",
    "    for sm in samples:\n",
    "        if quota_exhausted:\n",
    "            print(f\"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—æ¡ç‚¹: {sm.id}ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ï¼‰\")\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'general_score': 0.0,\n",
    "                'specific_score': 0.0,\n",
    "                'total_score': 0.0,\n",
    "                'notes': 'skipped_due_to_quota',\n",
    "            })\n",
    "            continue\n",
    "        steps = proc_map.get(sm.id, [])\n",
    "        msgs = build_judge_messages(sm, steps)\n",
    "        try:\n",
    "            completion = client.chat.completions.parse(\n",
    "                model=JUDGE_MODEL,\n",
    "                messages=msgs,\n",
    "                temperature=JUDGE_TEMPERATURE,\n",
    "                response_format=JudgeOutput,\n",
    "            )\n",
    "            parsed: JudgeOutput = completion.choices[0].message.parsed  # type: ignore\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'general_score': parsed.general_score,\n",
    "                'specific_score': parsed.specific_score,\n",
    "                'total_score': parsed.final_score,\n",
    "                'notes': parsed.notes or '',\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è©•ä¾¡å¤±æ•—: {sm.id}: {e}\")\n",
    "            if _is_insufficient_quota(e):\n",
    "                print(\"âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€ä»¥é™ã®æ¡ç‚¹ã‚’ä¸­æ–­ã—ã¾ã™ã€‚ãƒ—ãƒ©ãƒ³/èª²é‡‘è¨­å®šã‚’ã”ç¢ºèªãã ã•ã„ã€‚\")\n",
    "                quota_exhausted = True\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'general_score': 0.0,\n",
    "                'specific_score': 0.0,\n",
    "                'total_score': 0.0,\n",
    "                'notes': 'evaluation_failed',\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "df = judge_with_llm(samples, generated_results)\n",
    "print(f\"âœ… LLM-as-a-judge: Scored {len(df)} samples (0-10)\")\n",
    "try:\n",
    "    display(df[['id','general_score','specific_score','total_score']])\n",
    "except Exception:\n",
    "    print(df[['id','general_score','specific_score','total_score']])\n",
    "\n",
    "csv_path = out_dir / f'eval_{JUDGE_MODEL}_{ts}.csv'\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf_8_sig\")\n",
    "print(f'ğŸ“„ Saved: {csv_path}')\n",
    "\n",
    "try:\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’å‡ºã—ã¦ã€yãªã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    print(f\"Download file: {csv_path}\")\n",
    "    confirm = eval_js('confirm(\"ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ\")')\n",
    "    if confirm:\n",
    "      colab_files.download(str(csv_path))\n",
    "    else:\n",
    "      print(\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "except Exception:\n",
    "    display(FileLink(str(csv_path.resolve())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼\n",
    "#@title 8. è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ { display-mode: \"form\" }\n",
    "\n",
    "# 3ç¨®é¡ã®è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨è©•ä¾¡ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "# Schemaâ€‘Strict Checklist Judge (Japanese)\n",
    "STRICT_CHECKLIST_PROMPT = \"\"\"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã‚ã‚Šã€å³æ ¼ãªæ¡ç‚¹è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸ Inputï¼ˆinstruction, mandatory_objects, source_protocol_steps, expected_final_states, referencesï¼‰ã¨ã€å‚åŠ è€…ã® Outputï¼ˆprocedure_stepsï¼‰ãŠã‚ˆã³å•é¡Œã”ã¨ã® specific_criteria ã‚’è©•ä¾¡ã—ã€JudgeOutput ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å®Œå…¨æº–æ‹ ã—ã¾ã™ï¼šgeneral_score, specific_score, final_score, general_reason, specific_matches, notesã€‚ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n",
    "\n",
    "<è©•ä¾¡æ–¹é‡>\n",
    "- æ±ºå®šã¯äºŒå€¤ï¼ˆOK/NGï¼‰ã€‚ä¸ç¢ºå®Ÿãªå ´åˆã¯ NGï¼ˆç„¡åŠ ç‚¹ï¼‰ã€‚æ¨æ¸¬ãƒ»è£œå®Œã¯ã—ãªã„ã€‚\n",
    "- æ ¹æ‹ ã¯å‚åŠ è€… Output ã®è¨˜è¿°ã®ã¿ã‹ã‚‰å¼•ç”¨ã—ã€ã€Œã€ã§å›²ã‚€ã€‚Input ã¯ç…§åˆã«ã®ã¿ç”¨ã„ã€å¼•ç”¨ã—ãªã„ã€‚\n",
    "- general_reason ã¯å¿…ãš5è¡Œã€‚ã€Œ1)ï½5) åˆ¤å®šï¼ˆOK/NGï¼‰ | æ ¹æ‹  | å¼•ç”¨ã€ã€‚OKã®æ•°ã¯general_scoreã¨ä¸€è‡´ã•ã›ã‚‹ã€‚\n",
    "- final_score = general_score + specific_scoreã€‚æ•´æ•°ã€‚ç¯„å›² 0â€“10ã€‚\n",
    "- éåº¦ã®å®‰å…¨æ€§ã®ä¸Šé™ï¼ˆcapï¼‰ã¯general_scoreç®—å‡ºå¾Œã«é©ç”¨ã—ã€notesã«ç°¡æ½”ã«è¨˜è¼‰ã€‚ãªã„å ´åˆã¯ç©ºæ–‡å­—ã€‚\n",
    "\n",
    "<æ‰‹é †>\n",
    "1. Input ã‹ã‚‰å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ç‰©å“ãƒ»æ‰‹é †è«–ç†ãƒ»æœŸå¾…æœ€çµ‚çŠ¶æ…‹ã‚’æŠ½å‡ºã€‚\n",
    "2. Output ã‚’èµ°æŸ»ã—ã€æ“ä½œå¯èƒ½ãªå˜ä½ï¼ˆÂµL/mL, mg/g ç­‰ï¼‰ãƒ»å™¨å…·æ˜ç¤ºãƒ»é †åº/åˆ†å²ãƒ»è¨ˆç®—æ•´åˆæ€§ã®æœ‰ç„¡ã‚’ç¢ºèªã€‚\n",
    "3. 5ã¤ã®å…±é€šæ¡ç‚¹åŸºæº–ã‚’ç‹¬ç«‹ã«åˆ¤å®šï¼ˆOK/NGï¼‰ã€‚æ ¹æ‹ ã¯ Output ã®å…·ä½“ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å¼•ç”¨ã€‚\n",
    "4. specific_criteria ã‚’æº€ãŸã™ç®‡æ‰€ã®ã¿ specific_matches ã«å¼•ç”¨ã‚’è¨˜éŒ²ã—ã€è©²å½“ç‚¹ã‚’åŠ ç®—ã€‚æ›–æ˜§ãªã‚‰0ã€‚\n",
    "5. éåº¦ã®å®‰å…¨æ€§ã®æ¡ä»¶ã«è©²å½“ã™ã‚‹å ´åˆã€general_score ã‚’æœ€å¤§2ã« capã€‚notes ã«ã€Œcapç†ç”±ã€ã‚’çŸ­ãè¨˜è¼‰ã€‚\n",
    "6. JudgeOutput ã‚’è¿”ã™ã€‚æ‰€å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã€‚\n",
    "\n",
    "# å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "## åŠ ç‚¹é …ç›®ï¼ˆå„1ç‚¹ï¼‰\n",
    "1. ã€Œå®Ÿé¨“æŒ‡ç¤ºã€ã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€ã™ã¹ã¦èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚å˜ãªã‚‹å†æ²ã¯èªã‚ãªã„ã€‚\n",
    "2. ã€Œä½¿ç”¨ã™ã‚‹ç‰©å“ã€ã®è¦ç´ ãŒã€ã™ã¹ã¦ èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚æ›–æ˜§ãªç·ç§°ã®ã¿ã¯èªã‚ãªã„ã€‚\n",
    "3. ã€Œå…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ã€ã®é †åºã‚„åˆ†å²æ¡ä»¶ã®è«–ç†æ§‹é€ ã‚’çŸ›ç›¾ãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚\n",
    "4. å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ã€ŒæœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€ã‚’å¾—ã‚‰ã‚Œã‚‹ã€‚è«–ç†çš„ç ´ç¶»ã‚„è¨ˆç®—ãƒŸã‚¹ãŒãªã„ã€‚\n",
    "5. æ˜ç¤ºã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã‚’é©åˆ‡ã«è£œå®Œã—ã¦ã„ã‚‹ã€‚è¨˜è¿°ãŒå…·ä½“çš„ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "## æ¸›ç‚¹é …ç›®ï¼ˆå„-1ç‚¹ï¼‰\n",
    "1. éåº¦ã«ä¸è‡ªç„¶ãªæ—¥æœ¬èªã‚„ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã€‚\n",
    "2. è¨ˆç®—ãƒŸã‚¹ãŒã‚ã‚‹ã€‚\n",
    "3. æ‰‹é †ã«çŸ›ç›¾ãŒã‚ã‚‹ã€‚\n",
    "\n",
    "## éåº¦ã®å®‰å…¨æ€§\n",
    "ä»¥ä¸‹ã®ã‚ˆã†ã«éåº¦ã«å®‰å…¨å´ã«å€’ã—ãŸå›ç­”ã®å ´åˆã€å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreï¼‰ã®åˆè¨ˆç‚¹ã‚’æœ€å¤§2ç‚¹ã¨ã™ã‚‹ã€‚\n",
    "- å…¥åŠ›ã®æ‰‹é †ã‚’ãã®ã¾ã¾å‡ºåŠ›ã¨ã—ã¦è¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å‡ºå…¸ã®æƒ…å ±ã‚’é©åˆ‡ãªå–æ¨é¸æŠãªãè¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿé¨“æŒ‡ç¤ºã‚„å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«åã—ã¦ä¸­é–“ç‰©ã‚’å¿…è¦é‡ã®100å€ç”¨æ„ã™ã‚‹ãªã©ã€ã¿ã ã‚Šã«å®Ÿé¨“æ¡ä»¶ã‚’å®‰å…¨å´ã«å€’ã—ã¦ã„ã‚‹ã€‚\n",
    "### é©ç”¨ä¾‹\n",
    "- æ¡ç‚¹çµæœãŒ1ç‚¹ã®å ´åˆ â†’ 1ç‚¹ã®ã¾ã¾\n",
    "- æ¡ç‚¹çµæœãŒ3ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "- æ¡ç‚¹çµæœãŒ5ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "\n",
    "# å€‹åˆ¥æ¡ç‚¹åŸºæº–ï¼ˆspecific_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "å•é¡Œã”ã¨ã«è¨­å®šã•ã‚ŒãŸæ¡ç‚¹é …ç›®ã«åŸºã¥ã„ã¦åŠ ç‚¹ã™ã‚‹ã€‚æ¡ç‚¹é …ç›®ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªè¦³ç‚¹ã§è¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å‡ºé¡Œæ„å›³ã¸ã®é©åˆæ€§\n",
    "- å®‰å…¨æ€§ã®è€ƒæ…®\n",
    "- ã‚³ã‚¹ãƒˆåŠ¹ç‡\n",
    "- ä½œæ¥­åŠ¹ç‡\n",
    "- å®Ÿé¨“ç²¾åº¦å‘ä¸Šã¸ã®è²¢çŒ®\n",
    "ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ãŒæ¡ç‚¹é …ç›®ã®æ¡ä»¶ã‚’æ˜ç¤ºçš„ã«æº€ãŸã™å ´åˆã®ã¿ã€è©²å½“ã™ã‚‹ score ã‚’åŠ ç‚¹ã—ã€æ ¹æ‹ ã¨ãªã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’specific_matchesã«è¨˜éŒ²ã™ã‚‹ã€‚æ›–æ˜§ãªè¨˜è¿°ã¯åŠ ç‚¹ã—ãªã„ã€‚\n",
    "\n",
    "# å®Ÿé¨“æ‰‹é †ã®è©³ç´°åº¦ã«é–¢ã™ã‚‹å¿…é ˆè¦ä»¶\n",
    "å®Ÿé¨“æ‰‹é †ï¼ˆprocedure_stepsï¼‰ã¯ã€å˜ã«ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ§‹é€ åŒ–ã™ã‚‹ã ã‘ã§ã¯ãªãã€å½“è©²åˆ†é‡ã®å­¦éƒ¨ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å—ã‘ãŸäººãŒèª­ã‚“ã§èª¤è§£ãªãå®Ÿè¡Œã§ãã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚ãã®ãŸã‚ã«ã€ä»¥ä¸‹ã®ç‚¹ã‚’éµå®ˆã—ã¦ãã ã•ã„ã€‚\n",
    "- ã™ã¹ã¦ã®æ¡ä»¶ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- ä¸å®Œå…¨ãªè¨˜è¿°ãŒå®Œæˆã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å…·ä½“çš„ãªæ•°å€¤ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆÂµL/mL, mg/g ãªã©å®Ÿéš›ã«æ“ä½œã§ãã‚‹å˜ä½ã€‚pmol, ng ç­‰ã®ã¿ã®æç¤ºã¯ä¸å¯ã€‚æ›ç®—ã—ã¦ä½“ç©ã‚„è³ªé‡ã‚’ç¤ºã—ãŸå ´åˆã®ã¿å¯ï¼‰ã€‚\n",
    "- æ“ä½œå¯¾è±¡ã‚„å™¨å…·ã€è£…ç½®ãªã©å¯¾è±¡ç‰©ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\"\"\"\n",
    "\n",
    "# GPTâ€‘5â€‘Tuned Agentic Judge (English)\n",
    "GPT_5_TUNED_AGENTIC_JUDGE_PROMPT = \"\"\"You are a lifeâ€‘science expert and a strict grader. Evaluate the participantâ€™s Output (procedure_steps) against the competition Input (instruction, mandatory_objects, source_protocol_steps, expected_final_states, references) and problemâ€‘specific criteria. Return ONLY the JudgeOutput fields: general_score, specific_score, final_score, general_reason, specific_matches, notes. No extra text.\n",
    "\n",
    "<persistence>\n",
    "- Operate autonomously until evaluation is fully complete. Do NOT ask for clarification.\n",
    "- Under uncertainty, bias to NG (no credit); proceed and document decisions.\n",
    "</persistence>\n",
    "\n",
    "<tool_preamble>\n",
    "- Rephrase the goal briefly: â€œScore Output against Input using 5 general checks + specific criteria.â€\n",
    "- Plan: (1) Extract Input requirements; (2) parse Output; (3) apply five binary checks with quoted evidence; (4) apply overâ€‘safety cap if triggered; (5) collect specific_matches; (6) compute scores; (7) emit JudgeOutput.\n",
    "- Stop when: all five checks decided; specific_matches compiled; schema validated.\n",
    "</tool_preamble>\n",
    "\n",
    "<steering>\n",
    "- reasoning_effort: medium (use minimal for latencyâ€‘sensitive runs).\n",
    "- verbosity: low. No commentary beyond required fields.\n",
    "- Determinism: low temperature; avoid emitting any fields beyond schema.\n",
    "</steering>\n",
    "\n",
    "<general_checks_definition>\n",
    "1) Instruction parameters must be fully reflected AND operationalized (units/instruments). Restatement alone â†’ NG.\n",
    "2) Mandatory objects: ALL items concretely used; umbrella categories alone â†’ NG.\n",
    "3) Protocol logic: preserve order, branches, dependencies from source_protocol_steps without contradiction.\n",
    "4) Achievability: steps logically yield expected_final_states; no calculation or logical errors.\n",
    "5) Completeness/specificity: fill gaps appropriately; actionable units (ÂµL/mL, mg/g) and instruments present. Nonâ€‘operational units alone (e.g., only ng/pmol) â†’ NG unless converted.\n",
    "</general_checks_definition>\n",
    "\n",
    "<evidence_rules>\n",
    "- Quote ONLY from Output for general_reason and specific_matches, wrapped in ã€Œã€.\n",
    "- Use Input for alignment checks only; do not quote it.\n",
    "- If Output lacks necessary evidence, mark NG; do not invent.\n",
    "</evidence_rules>\n",
    "\n",
    "<over_safety_cap>\n",
    "Apply after computing general_score; cap to max 2 if:\n",
    "- Output copies Input steps verbatim without operationalization.\n",
    "- Output pastes references/external content without selection.\n",
    "- Output inflates quantities (e.g., Ã—100 intermediates) contrary to Input/protocol without justification.\n",
    "Record cap briefly in notes; otherwise notes is empty.\n",
    "</over_safety_cap>\n",
    "\n",
    "<format_strictness>\n",
    "- Emit ONLY: general_score, specific_score, final_score, general_reason (exactly 5 lines â€œ1)â€¦5) åˆ¤å®šï¼ˆOK/NGï¼‰ | æ ¹æ‹  | å¼•ç”¨â€), specific_matches (list of quoted Output phrases), notes.\n",
    "- Ensure OK count equals general_score; final_score equals sum.\n",
    "</format_strictness>\n",
    "\n",
    "# å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "## åŠ ç‚¹é …ç›®ï¼ˆå„1ç‚¹ï¼‰\n",
    "1. ã€Œå®Ÿé¨“æŒ‡ç¤ºã€ã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€ã™ã¹ã¦èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚å˜ãªã‚‹å†æ²ã¯èªã‚ãªã„ã€‚\n",
    "2. ã€Œä½¿ç”¨ã™ã‚‹ç‰©å“ã€ã®è¦ç´ ãŒã€ã™ã¹ã¦ èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚æ›–æ˜§ãªç·ç§°ã®ã¿ã¯èªã‚ãªã„ã€‚\n",
    "3. ã€Œå…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ã€ã®é †åºã‚„åˆ†å²æ¡ä»¶ã®è«–ç†æ§‹é€ ã‚’çŸ›ç›¾ãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚\n",
    "4. å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ã€ŒæœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€ã‚’å¾—ã‚‰ã‚Œã‚‹ã€‚è«–ç†çš„ç ´ç¶»ã‚„è¨ˆç®—ãƒŸã‚¹ãŒãªã„ã€‚\n",
    "5. æ˜ç¤ºã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã‚’é©åˆ‡ã«è£œå®Œã—ã¦ã„ã‚‹ã€‚è¨˜è¿°ãŒå…·ä½“çš„ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "## æ¸›ç‚¹é …ç›®ï¼ˆå„-1ç‚¹ï¼‰\n",
    "1. éåº¦ã«ä¸è‡ªç„¶ãªæ—¥æœ¬èªã‚„ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã€‚\n",
    "2. è¨ˆç®—ãƒŸã‚¹ãŒã‚ã‚‹ã€‚\n",
    "3. æ‰‹é †ã«çŸ›ç›¾ãŒã‚ã‚‹ã€‚\n",
    "\n",
    "## éåº¦ã®å®‰å…¨æ€§\n",
    "ä»¥ä¸‹ã®ã‚ˆã†ã«éåº¦ã«å®‰å…¨å´ã«å€’ã—ãŸå›ç­”ã®å ´åˆã€å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreï¼‰ã®åˆè¨ˆç‚¹ã‚’æœ€å¤§2ç‚¹ã¨ã™ã‚‹ã€‚\n",
    "- å…¥åŠ›ã®æ‰‹é †ã‚’ãã®ã¾ã¾å‡ºåŠ›ã¨ã—ã¦è¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å‡ºå…¸ã®æƒ…å ±ã‚’é©åˆ‡ãªå–æ¨é¸æŠãªãè¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿé¨“æŒ‡ç¤ºã‚„å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«åã—ã¦ä¸­é–“ç‰©ã‚’å¿…è¦é‡ã®100å€ç”¨æ„ã™ã‚‹ãªã©ã€ã¿ã ã‚Šã«å®Ÿé¨“æ¡ä»¶ã‚’å®‰å…¨å´ã«å€’ã—ã¦ã„ã‚‹ã€‚\n",
    "### é©ç”¨ä¾‹\n",
    "- æ¡ç‚¹çµæœãŒ1ç‚¹ã®å ´åˆ â†’ 1ç‚¹ã®ã¾ã¾\n",
    "- æ¡ç‚¹çµæœãŒ3ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "- æ¡ç‚¹çµæœãŒ5ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "\n",
    "# å€‹åˆ¥æ¡ç‚¹åŸºæº–ï¼ˆspecific_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "å•é¡Œã”ã¨ã«è¨­å®šã•ã‚ŒãŸæ¡ç‚¹é …ç›®ã«åŸºã¥ã„ã¦åŠ ç‚¹ã™ã‚‹ã€‚æ¡ç‚¹é …ç›®ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªè¦³ç‚¹ã§è¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å‡ºé¡Œæ„å›³ã¸ã®é©åˆæ€§\n",
    "- å®‰å…¨æ€§ã®è€ƒæ…®\n",
    "- ã‚³ã‚¹ãƒˆåŠ¹ç‡\n",
    "- ä½œæ¥­åŠ¹ç‡\n",
    "- å®Ÿé¨“ç²¾åº¦å‘ä¸Šã¸ã®è²¢çŒ®\n",
    "ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ãŒæ¡ç‚¹é …ç›®ã®æ¡ä»¶ã‚’æ˜ç¤ºçš„ã«æº€ãŸã™å ´åˆã®ã¿ã€è©²å½“ã™ã‚‹ score ã‚’åŠ ç‚¹ã—ã€æ ¹æ‹ ã¨ãªã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’specific_matchesã«è¨˜éŒ²ã™ã‚‹ã€‚æ›–æ˜§ãªè¨˜è¿°ã¯åŠ ç‚¹ã—ãªã„ã€‚\n",
    "\n",
    "# å®Ÿé¨“æ‰‹é †ã®è©³ç´°åº¦ã«é–¢ã™ã‚‹å¿…é ˆè¦ä»¶\n",
    "å®Ÿé¨“æ‰‹é †ï¼ˆprocedure_stepsï¼‰ã¯ã€å˜ã«ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ§‹é€ åŒ–ã™ã‚‹ã ã‘ã§ã¯ãªãã€å½“è©²åˆ†é‡ã®å­¦éƒ¨ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å—ã‘ãŸäººãŒèª­ã‚“ã§èª¤è§£ãªãå®Ÿè¡Œã§ãã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚ãã®ãŸã‚ã«ã€ä»¥ä¸‹ã®ç‚¹ã‚’éµå®ˆã—ã¦ãã ã•ã„ã€‚\n",
    "- ã™ã¹ã¦ã®æ¡ä»¶ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- ä¸å®Œå…¨ãªè¨˜è¿°ãŒå®Œæˆã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å…·ä½“çš„ãªæ•°å€¤ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆÂµL/mL, mg/g ãªã©å®Ÿéš›ã«æ“ä½œã§ãã‚‹å˜ä½ã€‚pmol, ng ç­‰ã®ã¿ã®æç¤ºã¯ä¸å¯ã€‚æ›ç®—ã—ã¦ä½“ç©ã‚„è³ªé‡ã‚’ç¤ºã—ãŸå ´åˆã®ã¿å¯ï¼‰ã€‚\n",
    "- æ“ä½œå¯¾è±¡ã‚„å™¨å…·ã€è£…ç½®ãªã©å¯¾è±¡ç‰©ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\"\"\"\n",
    "\n",
    "# Contrastive Dualâ€‘Pass Judge (Japanese)\n",
    "CONTRASTIVE_DUAL_PASS_PROMPT = \"\"\"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦åˆ†é‡ã®ã€Œå¯¾ç…§å‹ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‘ã‚¹ã€æ¡ç‚¹è€…ã§ã™ã€‚Inputï¼ˆinstruction, mandatory_objects, source_protocol_steps, expected_final_states, referencesï¼‰ã¨å‚åŠ è€… Outputï¼ˆprocedure_stepsï¼‰ã€specific_criteria ã‚’è©•ä¾¡ã—ã€JudgeOutput ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ä½™åˆ†ãªå‡ºåŠ›ã¯å³ç¦ã§ã™ã€‚\n",
    "\n",
    "<persistence>\n",
    "- ç›¸è«‡ã‚„ç¢ºèªã¯è¡Œã‚ãšã€è‡ªå¾‹çš„ã«è©•ä¾¡ã‚’å®Œäº†ã™ã‚‹ã€‚\n",
    "- ä¸ç¢ºå®Ÿãªå ´åˆã¯ NGï¼ˆç„¡åŠ ç‚¹ï¼‰ã€‚æ¨æ¸¬ã¯ã—ãªã„ã€‚åˆ¤æ–­ç†ç”±ã¯ Output ã®å¼•ç”¨ã§ç¤ºã™ã€‚\n",
    "</persistence>\n",
    "\n",
    "<è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ >\n",
    "- Pass Aï¼ˆFailureâ€‘firstï¼‰: å„å…±é€šåŸºæº–ã«ã¤ã„ã¦ã€NG ã¨ãªã‚‹æ ¹æ‹ ï¼ˆæ¬ è½ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æœªä½¿ç”¨ç‰©å“ã€é †åº/åˆ†å²ã®ç ´ç¶»ã€è¨ˆç®—/è«–ç†ã‚¨ãƒ©ãƒ¼ã€æ“ä½œä¸èƒ½ãªå˜ä½ã‚„å™¨å…·ã®ä¸åœ¨ï¼‰ã‚’ã€Output ã‹ã‚‰ã®ã¿æŠ½å‡ºãƒ»å¼•ç”¨ã€‚\n",
    "- Pass Bï¼ˆEvidenceâ€‘forâ€‘creditï¼‰: å„åŸºæº–ã®OKæ¡ä»¶ã‚’æº€ãŸã™ç›´æ¥çš„ãªè¨¼æ‹ ï¼ˆæ“ä½œå¯èƒ½ãªå˜ä½ã€å…·ä½“çš„å™¨å…·ã€å…¨ç‰©å“ã®ä½¿ç”¨ã€åˆ†å²ç¶­æŒã€æœ€çµ‚çŠ¶æ…‹ã®é”æˆï¼‰ã‚’ Output ã‹ã‚‰ã®ã¿æŠ½å‡ºãƒ»å¼•ç”¨ã€‚\n",
    "- è¡çªè§£æ±º: Pass A ãŒæœ‰åŠ¹ãªå¤±æ•—ç†ç”±ã‚’ç¤ºã—ãŸå ´åˆã€Pass B ãŒãã‚Œã‚’å®Œå…¨ã«æ‰“ã¡æ¶ˆã™æ˜ç¢ºãªå¼•ç”¨è¨¼æ‹ ã‚’ç¤ºã™ã¨ãã®ã¿ OKã€‚æ›–æ˜§ã¯ NGã€‚\n",
    "\n",
    "<å½¢å¼ãƒ»ç®—å®š>\n",
    "- general_reason ã¯å¿…ãš5è¡Œã§ã€Œ1)ï½5) åˆ¤å®šï¼ˆOK/NGï¼‰ | æ ¹æ‹  | å¼•ç”¨ã€ã€‚å¼•ç”¨ã¯ã€Œã€ã§å›²ã‚€ã€‚\n",
    "- specific_matches ã¯ã€æº€ãŸã—ãŸ specific_criteria ã”ã¨ã« Output ã®è©²å½“ãƒ•ãƒ¬ãƒ¼ã‚ºã®ã¿ã‚’åˆ—æŒ™ã€‚æ›–æ˜§ãªã‚‰è¿½åŠ ã—ãªã„ã€‚\n",
    "- final_score = general_score + specific_scoreã€‚éåº¦ã®å®‰å…¨æ€§ cap ã¯ general_score ç®—å‡ºå¾Œã«é©ç”¨ã—ã€notes ã«ç°¡æ½”ã«è¨˜è¼‰ã€‚ãªã‘ã‚Œã°ç©ºæ–‡å­—ã€‚\n",
    "- æ‰€å®šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆgeneral_score, specific_score, final_score, general_reason, specific_matches, notesï¼‰ä»¥å¤–ã¯å‡ºåŠ›ç¦æ­¢ã€‚\n",
    "\n",
    "# å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "## åŠ ç‚¹é …ç›®ï¼ˆå„1ç‚¹ï¼‰\n",
    "1. ã€Œå®Ÿé¨“æŒ‡ç¤ºã€ã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€ã™ã¹ã¦èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚å˜ãªã‚‹å†æ²ã¯èªã‚ãªã„ã€‚\n",
    "2. ã€Œä½¿ç”¨ã™ã‚‹ç‰©å“ã€ã®è¦ç´ ãŒã€ã™ã¹ã¦ èª¤ã‚Šãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚æ›–æ˜§ãªç·ç§°ã®ã¿ã¯èªã‚ãªã„ã€‚\n",
    "3. ã€Œå…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ã€ã®é †åºã‚„åˆ†å²æ¡ä»¶ã®è«–ç†æ§‹é€ ã‚’çŸ›ç›¾ãªãåæ˜ ã—ã¦ã„ã‚‹ã€‚\n",
    "4. å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ã€ŒæœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€ã‚’å¾—ã‚‰ã‚Œã‚‹ã€‚è«–ç†çš„ç ´ç¶»ã‚„è¨ˆç®—ãƒŸã‚¹ãŒãªã„ã€‚\n",
    "5. æ˜ç¤ºã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ã‚’é©åˆ‡ã«è£œå®Œã—ã¦ã„ã‚‹ã€‚è¨˜è¿°ãŒå…·ä½“çš„ã§ã‚ã‚‹ã€‚\n",
    "\n",
    "## æ¸›ç‚¹é …ç›®ï¼ˆå„-1ç‚¹ï¼‰\n",
    "1. éåº¦ã«ä¸è‡ªç„¶ãªæ—¥æœ¬èªã‚„ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ã€‚\n",
    "2. è¨ˆç®—ãƒŸã‚¹ãŒã‚ã‚‹ã€‚\n",
    "3. æ‰‹é †ã«çŸ›ç›¾ãŒã‚ã‚‹ã€‚\n",
    "\n",
    "## éåº¦ã®å®‰å…¨æ€§\n",
    "ä»¥ä¸‹ã®ã‚ˆã†ã«éåº¦ã«å®‰å…¨å´ã«å€’ã—ãŸå›ç­”ã®å ´åˆã€å…±é€šæ¡ç‚¹åŸºæº–ï¼ˆgeneral_scoreï¼‰ã®åˆè¨ˆç‚¹ã‚’æœ€å¤§2ç‚¹ã¨ã™ã‚‹ã€‚\n",
    "- å…¥åŠ›ã®æ‰‹é †ã‚’ãã®ã¾ã¾å‡ºåŠ›ã¨ã—ã¦è¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å‡ºå…¸ã®æƒ…å ±ã‚’é©åˆ‡ãªå–æ¨é¸æŠãªãè¨˜è¼‰ã—ã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿé¨“æŒ‡ç¤ºã‚„å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«åã—ã¦ä¸­é–“ç‰©ã‚’å¿…è¦é‡ã®100å€ç”¨æ„ã™ã‚‹ãªã©ã€ã¿ã ã‚Šã«å®Ÿé¨“æ¡ä»¶ã‚’å®‰å…¨å´ã«å€’ã—ã¦ã„ã‚‹ã€‚\n",
    "### é©ç”¨ä¾‹\n",
    "- æ¡ç‚¹çµæœãŒ1ç‚¹ã®å ´åˆ â†’ 1ç‚¹ã®ã¾ã¾\n",
    "- æ¡ç‚¹çµæœãŒ3ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "- æ¡ç‚¹çµæœãŒ5ç‚¹ã®å ´åˆ â†’ 2ç‚¹ã«åˆ¶é™\n",
    "\n",
    "# å€‹åˆ¥æ¡ç‚¹åŸºæº–ï¼ˆspecific_scoreã€æœ€å¤§5ç‚¹ï¼‰\n",
    "å•é¡Œã”ã¨ã«è¨­å®šã•ã‚ŒãŸæ¡ç‚¹é …ç›®ã«åŸºã¥ã„ã¦åŠ ç‚¹ã™ã‚‹ã€‚æ¡ç‚¹é …ç›®ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªè¦³ç‚¹ã§è¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å‡ºé¡Œæ„å›³ã¸ã®é©åˆæ€§\n",
    "- å®‰å…¨æ€§ã®è€ƒæ…®\n",
    "- ã‚³ã‚¹ãƒˆåŠ¹ç‡\n",
    "- ä½œæ¥­åŠ¹ç‡\n",
    "- å®Ÿé¨“ç²¾åº¦å‘ä¸Šã¸ã®è²¢çŒ®\n",
    "ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ãŒæ¡ç‚¹é …ç›®ã®æ¡ä»¶ã‚’æ˜ç¤ºçš„ã«æº€ãŸã™å ´åˆã®ã¿ã€è©²å½“ã™ã‚‹ score ã‚’åŠ ç‚¹ã—ã€æ ¹æ‹ ã¨ãªã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’specific_matchesã«è¨˜éŒ²ã™ã‚‹ã€‚æ›–æ˜§ãªè¨˜è¿°ã¯åŠ ç‚¹ã—ãªã„ã€‚\n",
    "\n",
    "# å®Ÿé¨“æ‰‹é †ã®è©³ç´°åº¦ã«é–¢ã™ã‚‹å¿…é ˆè¦ä»¶\n",
    "å®Ÿé¨“æ‰‹é †ï¼ˆprocedure_stepsï¼‰ã¯ã€å˜ã«ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ§‹é€ åŒ–ã™ã‚‹ã ã‘ã§ã¯ãªãã€å½“è©²åˆ†é‡ã®å­¦éƒ¨ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å—ã‘ãŸäººãŒèª­ã‚“ã§èª¤è§£ãªãå®Ÿè¡Œã§ãã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚ãã®ãŸã‚ã«ã€ä»¥ä¸‹ã®ç‚¹ã‚’éµå®ˆã—ã¦ãã ã•ã„ã€‚\n",
    "- ã™ã¹ã¦ã®æ¡ä»¶ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- ä¸å®Œå…¨ãªè¨˜è¿°ãŒå®Œæˆã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "- å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹å…·ä½“çš„ãªæ•°å€¤ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆÂµL/mL, mg/g ãªã©å®Ÿéš›ã«æ“ä½œã§ãã‚‹å˜ä½ã€‚pmol, ng ç­‰ã®ã¿ã®æç¤ºã¯ä¸å¯ã€‚æ›ç®—ã—ã¦ä½“ç©ã‚„è³ªé‡ã‚’ç¤ºã—ãŸå ´åˆã®ã¿å¯ï¼‰ã€‚\n",
    "- æ“ä½œå¯¾è±¡ã‚„å™¨å…·ã€è£…ç½®ãªã©å¯¾è±¡ç‰©ãŒæ˜ç¤ºåŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚\"\"\"\n",
    "\n",
    "PROMPT_VARIANTS = {\n",
    "    \"strict_checklist\": STRICT_CHECKLIST_PROMPT,\n",
    "    \"gpt5_tuned\": GPT_5_TUNED_AGENTIC_JUDGE_PROMPT,\n",
    "    \"contrastive_dual_pass\": CONTRASTIVE_DUAL_PASS_PROMPT,\n",
    "}\n",
    "\n",
    "PROMPT_VARIANT_LABELS = {\n",
    "    \"strict_checklist\": \"Prompt 1: å³æ ¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\",\n",
    "    \"gpt5_tuned\": \"Prompt 2: GPT-5-Tuned\",\n",
    "    \"contrastive_dual_pass\": \"Prompt 3: å¯¾ç…§å‹ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‘ã‚¹\",\n",
    "}\n",
    "\n",
    "\n",
    "def compose_judge_user_content(sample: ExampleSample, steps: List[Step]) -> str:\n",
    "    parts: List[str] = []\n",
    "    parts.append(f\"# Input\\n## å®Ÿé¨“æŒ‡ç¤º (instruction)\\n{sample.input.instruction}\")\n",
    "    if sample.input.mandatory_objects:\n",
    "        parts.append(\"\\n## ä½¿ç”¨ã™ã‚‹ç‰©å“ (mandatory_objects)\")\n",
    "        for it in sorted(sample.input.mandatory_objects):\n",
    "            parts.append(f\"- {it}\")\n",
    "    if sample.input.source_protocol_steps:\n",
    "        parts.append(\"\\n## å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ï¼ˆå‚è€ƒï¼‰(source_protocol_steps)\")\n",
    "        for st in sample.input.source_protocol_steps:\n",
    "            parts.append(f\"- {st.id}. {st.text}\")\n",
    "    if sample.input.expected_final_states:\n",
    "        parts.append(\"\\n## æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ (expected_final_states)\")\n",
    "        for fs in sorted(sample.input.expected_final_states):\n",
    "            parts.append(f\"- {fs}\")\n",
    "    if sample.input.references:\n",
    "        parts.append(\"\\n## å‚è€ƒæ–‡çŒ® (references)\")\n",
    "        for ref in sample.input.references:\n",
    "            parts.append(f\"- [{ref.id}] {ref.text}\")\n",
    "    parts.append(\"\\n# ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“æ‰‹é †ï¼ˆOutputï¼‰\")\n",
    "    for stp in steps:\n",
    "        parts.append(f\"- {stp.id}. {stp.text}\")\n",
    "    parts.append(\"\\n# specific_criteria\")\n",
    "    if sample.measurement and sample.measurement.specific_criteria:\n",
    "        for item, sc in sample.measurement.specific_criteria.items():\n",
    "            parts.append(f\"- ({int(sc)}ç‚¹): {item}\")\n",
    "    else:\n",
    "        parts.append(\"- ãªã—\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def build_judge_messages_with_prompt(prompt_key: str, sample: ExampleSample, steps: List[Step]) -> list[dict]:\n",
    "    if prompt_key not in PROMPT_VARIANTS:\n",
    "        raise ValueError(f\"æœªçŸ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ¼ã§ã™: {prompt_key}\")\n",
    "    system = PROMPT_VARIANTS[prompt_key]\n",
    "    user = compose_judge_user_content(sample, steps)\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "def ensure_evaluation_inputs_ready() -> None:\n",
    "    missing = [name for name in (\"samples\", \"generated_results\", \"out_dir\", \"ts\") if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"è©•ä¾¡ã«å¿…è¦ãªå¤‰æ•°ãŒæœªå®šç¾©ã§ã™: {', '.join(missing)}\")\n",
    "\n",
    "def judge_with_prompt(prompt_key: str, samples: List[ExampleSample], generated: list[dict], *, model: str = JUDGE_MODEL, temperature: float = JUDGE_TEMPERATURE) -> pd.DataFrame:\n",
    "    if prompt_key not in PROMPT_VARIANTS:\n",
    "        raise ValueError(f\"æœªçŸ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ¼ã§ã™: {prompt_key}\")\n",
    "    client = OpenAI(api_key=API_KEY) if 'API_KEY' in globals() and API_KEY else OpenAI()\n",
    "    proc_map = {g['id']: [Step(id=it['id'], text=it['text']) for it in g.get('procedure_steps', [])] for g in generated}\n",
    "    rows: List[dict] = []\n",
    "    quota_exhausted = False\n",
    "\n",
    "    def _is_insufficient_quota(err: Exception) -> bool:\n",
    "        s = str(err).lower()\n",
    "        return 'insufficient_quota' in s or 'exceeded your current quota' in s\n",
    "\n",
    "    for sm in samples:\n",
    "        if quota_exhausted:\n",
    "            print(f\"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—æ¡ç‚¹({PROMPT_VARIANT_LABELS.get(prompt_key, prompt_key)}): {sm.id}ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ï¼‰\")\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'variant': prompt_key,\n",
    "                'general_score': 0.0,\n",
    "                'specific_score': 0.0,\n",
    "                'total_score': 0.0,\n",
    "                'general_reason': '',\n",
    "                'specific_matches': [],\n",
    "                'notes': 'skipped_due_to_quota',\n",
    "            })\n",
    "            continue\n",
    "        steps = proc_map.get(sm.id, [])\n",
    "        msgs = build_judge_messages_with_prompt(prompt_key, sm, steps)\n",
    "        try:\n",
    "            completion = client.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=msgs,\n",
    "                temperature=temperature,\n",
    "                response_format=JudgeOutput,\n",
    "            )\n",
    "            parsed: JudgeOutput = completion.choices[0].message.parsed  # type: ignore\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'variant': prompt_key,\n",
    "                'general_score': float(parsed.general_score),\n",
    "                'specific_score': float(parsed.specific_score),\n",
    "                'total_score': float(parsed.final_score),\n",
    "                'general_reason': parsed.general_reason,\n",
    "                'specific_matches': list(parsed.specific_matches),\n",
    "                'notes': parsed.notes or '',\n",
    "            })\n",
    "        except Exception as e:\n",
    "            label = PROMPT_VARIANT_LABELS.get(prompt_key, prompt_key)\n",
    "            print(f\"âŒ {label} è©•ä¾¡å¤±æ•—: {sm.id}: {e}\")\n",
    "            if _is_insufficient_quota(e):\n",
    "                print(\"âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€ä»¥é™ã®æ¡ç‚¹ã‚’ä¸­æ–­ã—ã¾ã™ã€‚ãƒ—ãƒ©ãƒ³/èª²é‡‘è¨­å®šã‚’ã”ç¢ºèªãã ã•ã„ã€‚\")\n",
    "                quota_exhausted = True\n",
    "            rows.append({\n",
    "                'id': sm.id,\n",
    "                'variant': prompt_key,\n",
    "                'general_score': 0.0,\n",
    "                'specific_score': 0.0,\n",
    "                'total_score': 0.0,\n",
    "                'general_reason': '',\n",
    "                'specific_matches': [],\n",
    "                'notes': 'evaluation_failed',\n",
    "            })\n",
    "    df_variant = pd.DataFrame(rows)\n",
    "    return df_variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Prompt 1: å³æ ¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:19:04 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:20:41 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:21:47 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:22:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:24:02 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:24:59 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:26:04 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:27:33 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:29:24 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:30:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt 1: å³æ ¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ: Scored 10 samples (0-10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "general_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "specific_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "dec18947-d6a0-4b0f-9886-3ee05687087a",
       "rows": [
        [
         "0",
         "private_test_1",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "1",
         "private_test_10",
         "3.0",
         "4.0",
         "7.0"
        ],
        [
         "2",
         "private_test_2",
         "4.0",
         "0.0",
         "4.0"
        ],
        [
         "3",
         "private_test_3",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "4",
         "private_test_4",
         "2.0",
         "4.0",
         "6.0"
        ],
        [
         "5",
         "private_test_5",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "6",
         "private_test_6",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "7",
         "private_test_7",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "8",
         "private_test_8",
         "1.0",
         "5.0",
         "6.0"
        ],
        [
         "9",
         "private_test_9",
         "4.0",
         "2.0",
         "6.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>general_score</th>\n",
       "      <th>specific_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private_test_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private_test_10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private_test_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private_test_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>private_test_4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>private_test_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>private_test_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>private_test_7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private_test_8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>private_test_9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  general_score  specific_score  total_score\n",
       "0   private_test_1            5.0             5.0         10.0\n",
       "1  private_test_10            3.0             4.0          7.0\n",
       "2   private_test_2            4.0             0.0          4.0\n",
       "3   private_test_3            4.0             4.0          8.0\n",
       "4   private_test_4            2.0             4.0          6.0\n",
       "5   private_test_5            4.0             4.0          8.0\n",
       "6   private_test_6            4.0             4.0          8.0\n",
       "7   private_test_7            5.0             5.0         10.0\n",
       "8   private_test_8            1.0             5.0          6.0\n",
       "9   private_test_9            4.0             2.0          6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Saved: outputs/runs/eval_gpt-5-mini-2025-08-07_strict_checklist_20251223_150826.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Prompt 1 ã§è©•ä¾¡\n",
    "#@title 9. è©•ä¾¡: Prompt 1ï¼ˆstrict_checklistï¼‰ { display-mode: \"form\" }\n",
    "\n",
    "ensure_evaluation_inputs_ready()\n",
    "variant_key = \"strict_checklist\"\n",
    "print(f\"â–¶ï¸ {PROMPT_VARIANT_LABELS[variant_key]} ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "\n",
    "df_strict_checklist = judge_with_prompt(variant_key, samples, generated_results)\n",
    "print(f\"âœ… {PROMPT_VARIANT_LABELS[variant_key]}: Scored {len(df_strict_checklist)} samples (0-10)\")\n",
    "try:\n",
    "    display(df_strict_checklist[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "except Exception:\n",
    "    print(df_strict_checklist[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "\n",
    "csv_path_strict = out_dir / f'eval_{JUDGE_MODEL}_{variant_key}_{ts}.csv'\n",
    "df_strict_checklist.to_csv(csv_path_strict, index=False, encoding=\"utf_8_sig\")\n",
    "print(f\"ğŸ“„ Saved: {csv_path_strict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Prompt 2: GPT-5-Tuned ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:36:30 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:38:33 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:40:29 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:42:28 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:44:00 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:45:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:46:36 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:49:06 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:50:47 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:52:09 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt 2: GPT-5-Tuned: Scored 10 samples (0-10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "general_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "specific_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "befea609-331a-4e7a-b95f-35ae102b826e",
       "rows": [
        [
         "0",
         "private_test_1",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "1",
         "private_test_10",
         "4.0",
         "2.0",
         "6.0"
        ],
        [
         "2",
         "private_test_2",
         "3.0",
         "0.0",
         "3.0"
        ],
        [
         "3",
         "private_test_3",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "4",
         "private_test_4",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "5",
         "private_test_5",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "6",
         "private_test_6",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "7",
         "private_test_7",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "8",
         "private_test_8",
         "4.0",
         "2.0",
         "6.0"
        ],
        [
         "9",
         "private_test_9",
         "5.0",
         "3.0",
         "8.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>general_score</th>\n",
       "      <th>specific_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private_test_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private_test_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private_test_2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private_test_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>private_test_4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>private_test_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>private_test_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>private_test_7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private_test_8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>private_test_9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  general_score  specific_score  total_score\n",
       "0   private_test_1            5.0             5.0         10.0\n",
       "1  private_test_10            4.0             2.0          6.0\n",
       "2   private_test_2            3.0             0.0          3.0\n",
       "3   private_test_3            4.0             4.0          8.0\n",
       "4   private_test_4            5.0             5.0         10.0\n",
       "5   private_test_5            4.0             4.0          8.0\n",
       "6   private_test_6            4.0             4.0          8.0\n",
       "7   private_test_7            5.0             5.0         10.0\n",
       "8   private_test_8            4.0             2.0          6.0\n",
       "9   private_test_9            5.0             3.0          8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Saved: outputs/runs/eval_gpt-5-mini-2025-08-07_gpt5_tuned_20251223_150826.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Prompt 2 ã§è©•ä¾¡\n",
    "#@title 10. è©•ä¾¡: Prompt 2ï¼ˆgpt5_tunedï¼‰ { display-mode: \"form\" }\n",
    "\n",
    "ensure_evaluation_inputs_ready()\n",
    "variant_key = \"gpt5_tuned\"\n",
    "print(f\"â–¶ï¸ {PROMPT_VARIANT_LABELS[variant_key]} ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "\n",
    "df_gpt5_tuned = judge_with_prompt(variant_key, samples, generated_results)\n",
    "print(f\"âœ… {PROMPT_VARIANT_LABELS[variant_key]}: Scored {len(df_gpt5_tuned)} samples (0-10)\")\n",
    "try:\n",
    "    display(df_gpt5_tuned[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "except Exception:\n",
    "    print(df_gpt5_tuned[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "\n",
    "csv_path_gpt5 = out_dir / f'eval_{JUDGE_MODEL}_{variant_key}_{ts}.csv'\n",
    "df_gpt5_tuned.to_csv(csv_path_gpt5, index=False, encoding=\"utf_8_sig\")\n",
    "print(f\"ğŸ“„ Saved: {csv_path_gpt5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Prompt 3: å¯¾ç…§å‹ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‘ã‚¹ ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:53:10 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:54:56 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:56:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:57:46 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "15:58:56 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:00:30 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:02:29 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:03:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:05:11 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:07:24 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt 3: å¯¾ç…§å‹ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ‘ã‚¹: Scored 10 samples (0-10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "general_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "specific_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "05878725-f43c-4bc2-a79e-907874f81f54",
       "rows": [
        [
         "0",
         "private_test_1",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "1",
         "private_test_10",
         "4.0",
         "3.0",
         "7.0"
        ],
        [
         "2",
         "private_test_2",
         "4.0",
         "1.0",
         "5.0"
        ],
        [
         "3",
         "private_test_3",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "4",
         "private_test_4",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "5",
         "private_test_5",
         "3.0",
         "4.0",
         "7.0"
        ],
        [
         "6",
         "private_test_6",
         "4.0",
         "4.0",
         "8.0"
        ],
        [
         "7",
         "private_test_7",
         "5.0",
         "5.0",
         "10.0"
        ],
        [
         "8",
         "private_test_8",
         "3.0",
         "5.0",
         "8.0"
        ],
        [
         "9",
         "private_test_9",
         "4.0",
         "3.0",
         "7.0"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>general_score</th>\n",
       "      <th>specific_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private_test_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private_test_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private_test_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private_test_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>private_test_4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>private_test_5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>private_test_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>private_test_7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private_test_8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>private_test_9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  general_score  specific_score  total_score\n",
       "0   private_test_1            5.0             5.0         10.0\n",
       "1  private_test_10            4.0             3.0          7.0\n",
       "2   private_test_2            4.0             1.0          5.0\n",
       "3   private_test_3            4.0             4.0          8.0\n",
       "4   private_test_4            5.0             5.0         10.0\n",
       "5   private_test_5            3.0             4.0          7.0\n",
       "6   private_test_6            4.0             4.0          8.0\n",
       "7   private_test_7            5.0             5.0         10.0\n",
       "8   private_test_8            3.0             5.0          8.0\n",
       "9   private_test_9            4.0             3.0          7.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Saved: outputs/runs/eval_gpt-5-mini-2025-08-07_contrastive_dual_pass_20251223_150826.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Prompt 3 ã§è©•ä¾¡\n",
    "#@title 11. è©•ä¾¡: Prompt 3ï¼ˆcontrastive_dual_passï¼‰ { display-mode: \"form\" }\n",
    "\n",
    "ensure_evaluation_inputs_ready()\n",
    "variant_key = \"contrastive_dual_pass\"\n",
    "print(f\"â–¶ï¸ {PROMPT_VARIANT_LABELS[variant_key]} ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "\n",
    "df_contrastive_dual_pass = judge_with_prompt(variant_key, samples, generated_results)\n",
    "print(f\"âœ… {PROMPT_VARIANT_LABELS[variant_key]}: Scored {len(df_contrastive_dual_pass)} samples (0-10)\")\n",
    "try:\n",
    "    display(df_contrastive_dual_pass[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "except Exception:\n",
    "    print(df_contrastive_dual_pass[['id', 'general_score', 'specific_score', 'total_score']])\n",
    "\n",
    "csv_path_contrastive = out_dir / f'eval_{JUDGE_MODEL}_{variant_key}_{ts}.csv'\n",
    "df_contrastive_dual_pass.to_csv(csv_path_contrastive, index=False, encoding=\"utf_8_sig\")\n",
    "print(f\"ğŸ“„ Saved: {csv_path_contrastive}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Score summary (mean/std):\n",
      "      baseline_total  strict_checklist_total  gpt5_tuned_total  \\\n",
      "mean           8.300                   7.300             7.700   \n",
      "std            1.829                   1.889             2.214   \n",
      "\n",
      "      contrastive_dual_pass_total  \n",
      "mean                        8.000  \n",
      "std                         1.633  \n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "baseline_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "strict_checklist_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gpt5_tuned_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "contrastive_dual_pass_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff_strict_vs_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff_gpt5_vs_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff_contrastive_vs_baseline",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0c01a1a8-2af9-4eae-a2c5-db506ffac3f3",
       "rows": [
        [
         "0",
         "private_test_1",
         "10.0",
         "10.0",
         "10.0",
         "10.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "private_test_10",
         "6.0",
         "7.0",
         "6.0",
         "7.0",
         "1.0",
         "0.0",
         "1.0"
        ],
        [
         "2",
         "private_test_2",
         "5.0",
         "4.0",
         "3.0",
         "5.0",
         "-1.0",
         "-2.0",
         "0.0"
        ],
        [
         "3",
         "private_test_3",
         "9.0",
         "8.0",
         "8.0",
         "8.0",
         "-1.0",
         "-1.0",
         "-1.0"
        ],
        [
         "4",
         "private_test_4",
         "10.0",
         "6.0",
         "10.0",
         "10.0",
         "-4.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "private_test_5",
         "8.0",
         "8.0",
         "8.0",
         "7.0",
         "0.0",
         "0.0",
         "-1.0"
        ],
        [
         "6",
         "private_test_6",
         "10.0",
         "8.0",
         "8.0",
         "8.0",
         "-2.0",
         "-2.0",
         "-2.0"
        ],
        [
         "7",
         "private_test_7",
         "10.0",
         "10.0",
         "10.0",
         "10.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "private_test_8",
         "7.0",
         "6.0",
         "6.0",
         "8.0",
         "-1.0",
         "-1.0",
         "1.0"
        ],
        [
         "9",
         "private_test_9",
         "8.0",
         "6.0",
         "8.0",
         "7.0",
         "-2.0",
         "0.0",
         "-1.0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>baseline_total</th>\n",
       "      <th>strict_checklist_total</th>\n",
       "      <th>gpt5_tuned_total</th>\n",
       "      <th>contrastive_dual_pass_total</th>\n",
       "      <th>diff_strict_vs_baseline</th>\n",
       "      <th>diff_gpt5_vs_baseline</th>\n",
       "      <th>diff_contrastive_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private_test_1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private_test_10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private_test_2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private_test_3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>private_test_4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>private_test_5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>private_test_6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>private_test_7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private_test_8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>private_test_9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  baseline_total  strict_checklist_total  gpt5_tuned_total  \\\n",
       "0   private_test_1            10.0                    10.0              10.0   \n",
       "1  private_test_10             6.0                     7.0               6.0   \n",
       "2   private_test_2             5.0                     4.0               3.0   \n",
       "3   private_test_3             9.0                     8.0               8.0   \n",
       "4   private_test_4            10.0                     6.0              10.0   \n",
       "5   private_test_5             8.0                     8.0               8.0   \n",
       "6   private_test_6            10.0                     8.0               8.0   \n",
       "7   private_test_7            10.0                    10.0              10.0   \n",
       "8   private_test_8             7.0                     6.0               6.0   \n",
       "9   private_test_9             8.0                     6.0               8.0   \n",
       "\n",
       "   contrastive_dual_pass_total  diff_strict_vs_baseline  \\\n",
       "0                         10.0                      0.0   \n",
       "1                          7.0                      1.0   \n",
       "2                          5.0                     -1.0   \n",
       "3                          8.0                     -1.0   \n",
       "4                         10.0                     -4.0   \n",
       "5                          7.0                      0.0   \n",
       "6                          8.0                     -2.0   \n",
       "7                         10.0                      0.0   \n",
       "8                          8.0                     -1.0   \n",
       "9                          7.0                     -2.0   \n",
       "\n",
       "   diff_gpt5_vs_baseline  diff_contrastive_vs_baseline  \n",
       "0                    0.0                           0.0  \n",
       "1                    0.0                           1.0  \n",
       "2                   -2.0                           0.0  \n",
       "3                   -1.0                          -1.0  \n",
       "4                    0.0                           0.0  \n",
       "5                    0.0                          -1.0  \n",
       "6                   -2.0                          -2.0  \n",
       "7                    0.0                           0.0  \n",
       "8                   -1.0                           1.0  \n",
       "9                    0.0                          -1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 12: è©•ä¾¡çµæœã®æ¯”è¼ƒ\n",
    "#@title 12. æ–°æ—§è©•ä¾¡ã®æ¯”è¼ƒ { display-mode: \"form\" }\n",
    "\n",
    "required_frames = {\n",
    "    'baseline(df)': 'df' in globals(),\n",
    "    'strict_checklist': 'df_strict_checklist' in globals(),\n",
    "    'gpt5_tuned': 'df_gpt5_tuned' in globals(),\n",
    "    'contrastive_dual_pass': 'df_contrastive_dual_pass' in globals(),\n",
    "}\n",
    "missing = [name for name, available in required_frames.items() if not available]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"æ¯”è¼ƒã«ã¯æ¬¡ã®çµæœãŒå¿…è¦ã§ã™: {', '.join(missing)}\")\n",
    "\n",
    "comparison = df[['id', 'total_score']].rename(columns={'total_score': 'baseline_total'})\n",
    "variant_frames = {\n",
    "    'strict_checklist_total': df_strict_checklist[['id', 'total_score']].rename(columns={'total_score': 'strict_checklist_total'}),\n",
    "    'gpt5_tuned_total': df_gpt5_tuned[['id', 'total_score']].rename(columns={'total_score': 'gpt5_tuned_total'}),\n",
    "    'contrastive_dual_pass_total': df_contrastive_dual_pass[['id', 'total_score']].rename(columns={'total_score': 'contrastive_dual_pass_total'}),\n",
    "}\n",
    "for _, frame in variant_frames.items():\n",
    "    comparison = comparison.merge(frame, on='id', how='outer')\n",
    "\n",
    "comparison['diff_strict_vs_baseline'] = comparison['strict_checklist_total'] - comparison['baseline_total']\n",
    "comparison['diff_gpt5_vs_baseline'] = comparison['gpt5_tuned_total'] - comparison['baseline_total']\n",
    "comparison['diff_contrastive_vs_baseline'] = comparison['contrastive_dual_pass_total'] - comparison['baseline_total']\n",
    "\n",
    "score_cols = ['baseline_total', 'strict_checklist_total', 'gpt5_tuned_total', 'contrastive_dual_pass_total']\n",
    "summary = comparison[score_cols].agg(['mean', 'std']).round(3)\n",
    "print(\"ğŸ“Š Score summary (mean/std):\")\n",
    "print(summary)\n",
    "\n",
    "try:\n",
    "    display(comparison)\n",
    "except Exception:\n",
    "    print(comparison)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "la-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
