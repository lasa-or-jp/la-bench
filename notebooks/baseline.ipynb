{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b40ac992b90b4aaa88a76bfdfbdac9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97a08cebeac54ab09f411be9aba92df7",
              "IPY_MODEL_44cf68d6ead14da0aa7728d2dcd62dea",
              "IPY_MODEL_04780b69229e4843b6afe0749eea65f0"
            ],
            "layout": "IPY_MODEL_2f63e788f9cb4cee8c5f7a1bf3493dc2"
          }
        },
        "97a08cebeac54ab09f411be9aba92df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82338319c7243c2b140cab742e6cbf6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f35d254373ad43c9b9286d6f09c60cce",
            "value": "Processingâ€‡exampleâ€‡dataset:â€‡100%"
          }
        },
        "44cf68d6ead14da0aa7728d2dcd62dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59189fe39e3d4138ac6b2827be7c62e6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb000000b4984fba9929e157d7325495",
            "value": 2
          }
        },
        "04780b69229e4843b6afe0749eea65f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56626730a9a546908e25dee10e500ba3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef88243320c44924ac5e804ffee3069b",
            "value": "â€‡2/2â€‡[01:50&lt;00:00,â€‡53.95s/it]"
          }
        },
        "2f63e788f9cb4cee8c5f7a1bf3493dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82338319c7243c2b140cab742e6cbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35d254373ad43c9b9286d6f09c60cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59189fe39e3d4138ac6b2827be7c62e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb000000b4984fba9929e157d7325495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56626730a9a546908e25dee10e500ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef88243320c44924ac5e804ffee3069b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrV21bF8yUQL",
        "outputId": "42e683c8-bb8c-43f3-ef4a-5f7fd9f10e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Google Colaboratoryç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\n",
            "\n",
            "ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\n",
            "âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\n",
            "\n",
            "ğŸ“¥ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­: https://github.com/lasa-or-jp/la-bench.git\n",
            "âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†: la-bench/\n",
            "\n",
            "ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: /content/la-bench/la-bench/la-bench\n",
            "\n",
            "ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ :\n",
            "total 52\n",
            "drwxr-xr-x  8 root root 4096 Aug 10 09:36 .\n",
            "drwxr-xr-x 10 root root 4096 Aug 10 09:36 ..\n",
            "drwxr-xr-x  4 root root 4096 Aug 10 09:36 code\n",
            "-rw-r--r--  1 root root 2658 Aug 10 09:36 CONTRIBUTING.md\n",
            "drwxr-xr-x  4 root root 4096 Aug 10 09:36 data\n",
            "drwxr-xr-x  2 root root 4096 Aug 10 09:36 docs\n",
            "drwxr-xr-x  8 root root 4096 Aug 10 09:36 .git\n",
            "drwxr-xr-x  3 root root 4096 Aug 10 09:36 .github\n",
            "-rw-r--r--  1 root root  971 Aug 10 09:36 .gitignore\n",
            "-rw-r--r--  1 root root 1101 Aug 10 09:36 LICENSE\n",
            "-rw-r--r--  1 root root 4196 Aug 10 09:36 README.md\n",
            "drwxr-xr-x  2 root root 4096 Aug 10 09:36 submissions\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: ç’°å¢ƒè¨­å®šã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "\"\"\"\n",
        "LA-Bench 2025: å®Ÿé¨“æ‰‹é †ç”Ÿæˆã‚¿ã‚¹ã‚¯\n",
        "Baseline Implementation for Google Colaboratory\n",
        "GitHub: https://github.com/lasa-or-jp/la-bench.git\n",
        "\"\"\"\n",
        "\n",
        "#@title 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— { display-mode: \"form\" }\n",
        "#@markdown ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ã€‚\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Colabã‹ã©ã†ã‹ã®ç¢ºèª\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ… Google Colaboratoryç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\")\n",
        "\n",
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "print(\"\\nğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
        "!pip install -q openai tenacity pyyaml tqdm python-Levenshtein pandas numpy matplotlib seaborn\n",
        "\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
        "\n",
        "# GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³\n",
        "REPO_URL = \"https://github.com/lasa-or-jp/la-bench.git\"\n",
        "REPO_NAME = \"la-bench\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"\\nğŸ“¥ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­: {REPO_URL}\")\n",
        "    !git clone -q {REPO_URL}\n",
        "    print(f\"âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³å®Œäº†: {REPO_NAME}/\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒªã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {REPO_NAME}/\")\n",
        "    print(\"ğŸ“¥ æœ€æ–°ç‰ˆã«æ›´æ–°ä¸­...\")\n",
        "    !cd {REPO_NAME} && git pull -q\n",
        "    print(\"âœ… æ›´æ–°å®Œäº†\")\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š\n",
        "WORK_DIR = Path(REPO_NAME)\n",
        "os.chdir(WORK_DIR)\n",
        "print(f\"\\nğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ç¢ºèª\n",
        "print(\"\\nğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ :\")\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: OpenAI APIã‚­ãƒ¼ã®è¨­å®š\n",
        "#@title 2. OpenAI API Keyè¨­å®š { display-mode: \"form\" }\n",
        "#@markdown OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ã¯å®‰å…¨ã«ç®¡ç†ã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•ã‚’é¸æŠ\n",
        "use_secrets = True  #@param {type:\"boolean\"}\n",
        "#@markdown â˜ï¸ Google Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ãƒã‚§ãƒƒã‚¯\n",
        "\n",
        "if IN_COLAB:\n",
        "    if use_secrets:\n",
        "        try:\n",
        "            # Colab Secretsã‹ã‚‰å–å¾—\n",
        "            API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "            print(\"âœ… APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ\")\n",
        "        except Exception as e:\n",
        "            print(\"âš ï¸ Secretsã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "            print(\"å·¦å´ã®ãƒ‘ãƒãƒ«ã®ğŸ”‘ã‚¢ã‚¤ã‚³ãƒ³ã‹ã‚‰'OPENAI_API_KEY'ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
        "            API_KEY = None\n",
        "    else:\n",
        "        # ç›´æ¥å…¥åŠ›\n",
        "        api_key_input = getpass.getpass(\"ğŸ”‘ OpenAI API Keyã‚’å…¥åŠ›: \")\n",
        "        if api_key_input:\n",
        "            API_KEY = api_key_input\n",
        "            os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "            print(\"âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ\")\n",
        "        else:\n",
        "            API_KEY = None\n",
        "            print(\"âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ‰‹æ³•ã®ã¿ä½¿ç”¨ï¼‰\")\n",
        "else:\n",
        "    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
        "    API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not API_KEY:\n",
        "        API_KEY = input(\"OpenAI API Key: \")\n",
        "\n",
        "# APIã‚­ãƒ¼ã®æ¤œè¨¼\n",
        "if API_KEY:\n",
        "    print(f\"ğŸ”‘ APIã‚­ãƒ¼: {'*' * 20}{API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPTæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZr_6rXsyqlK",
        "outputId": "6ff0f0c2-3bb0-49b0-88cd-c7c4592ba502"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… APIã‚­ãƒ¼ã‚’Secretsã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ\n",
            "ğŸ”‘ APIã‚­ãƒ¼: ********************JacA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
        "#@title 3. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ { display-mode: \"form\" }\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field, asdict\n",
        "\n",
        "# OpenAI API\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(\"âš ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
        "\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
        "from difflib import SequenceMatcher\n",
        "try:\n",
        "    import Levenshtein\n",
        "    LEVENSHTEIN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LEVENSHTEIN_AVAILABLE = False\n",
        "\n",
        "# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ (Colabå¯¾å¿œ)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# å¯è¦–åŒ–\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "if IN_COLAB:\n",
        "    # Colabã§ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
        "    !apt-get -q install fonts-noto-cjk > /dev/null 2>&1\n",
        "    import matplotlib\n",
        "    import matplotlib.font_manager as fm\n",
        "\n",
        "    # ãƒ•ã‚©ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†æ§‹ç¯‰\n",
        "    # fontManagerã®ãƒ•ã‚©ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢\n",
        "    fm.fontManager.ttflist.clear()\n",
        "    fm.fontManager.afmlist.clear()\n",
        "\n",
        "    # fontManagerã‚’å†åˆæœŸåŒ–ï¼ˆæ–°ã—ã„ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œå‡ºï¼‰\n",
        "    fm.fontManager = fm.FontManager()\n",
        "\n",
        "    # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ³ãƒˆã‚’ç¢ºèª\n",
        "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "    japanese_fonts = [f for f in available_fonts if 'CJK' in f or 'Noto' in f]\n",
        "\n",
        "    if japanese_fonts:\n",
        "        print(f\"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œå‡º: {japanese_fonts[:3]}...\")  # æœ€åˆã®3ã¤ã ã‘è¡¨ç¤º\n",
        "\n",
        "        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š\n",
        "        plt.rcParams['font.family'] = 'sans-serif'\n",
        "        plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP'] + plt.rcParams['font.sans-serif']\n",
        "\n",
        "        # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®è¡¨ç¤ºè¨­å®š\n",
        "        plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "        print(\"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ\")\n",
        "    else:\n",
        "        print(\"âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
        "\n",
        "# ãƒ­ã‚°è¨­å®š\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LA-Bench 2025 Baseline Implementation\")\n",
        "print(f\"å®Ÿè¡Œç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"OpenAIåˆ©ç”¨å¯èƒ½: {OPENAI_AVAILABLE and API_KEY is not None}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71dPwmdnzEfP",
        "outputId": "ab7cc1b1-bc83-4b47-b0b4-a234d6203631"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œå‡º: ['Noto Sans CJK JP', 'Noto Serif CJK JP', 'Noto Sans CJK JP']...\n",
            "âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ\n",
            "============================================================\n",
            "LA-Bench 2025 Baseline Implementation\n",
            "å®Ÿè¡Œç’°å¢ƒ: Google Colab\n",
            "å®Ÿè¡Œæ™‚åˆ»: 2025-08-10 09:36:41\n",
            "OpenAIåˆ©ç”¨å¯èƒ½: True\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
        "#@title 4. ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å®šç¾© { display-mode: \"form\" }\n",
        "\n",
        "@dataclass\n",
        "class ProtocolStep:\n",
        "    \"\"\"ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®ã‚¹ãƒ†ãƒƒãƒ—\"\"\"\n",
        "    id: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ResourceInfo:\n",
        "    \"\"\"ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€ãƒãƒƒãƒ”ãƒ³ã‚°ã€åˆ¶ç´„ï¼‰\"\"\"\n",
        "    aliases: Dict[str, str] = field(default_factory=dict)\n",
        "    mappings: Dict[str, str] = field(default_factory=dict)\n",
        "    constraints: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "@dataclass\n",
        "class ExperimentTask:\n",
        "    \"\"\"å®Ÿé¨“ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ \"\"\"\n",
        "    id: str\n",
        "    instruction: str\n",
        "    protocol: List[ProtocolStep]\n",
        "    resources: ResourceInfo\n",
        "    ground_truth: Optional[List[ProtocolStep]] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, json_data: Dict) -> 'ExperimentTask':\n",
        "        \"\"\"JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ExperimentTaskã‚’ç”Ÿæˆ\"\"\"\n",
        "        protocol_steps = [\n",
        "            ProtocolStep(id=step['id'], text=step['text'])\n",
        "            for step in json_data['protocol']['steps']\n",
        "        ]\n",
        "\n",
        "        resources = ResourceInfo(\n",
        "            aliases=json_data['resources'].get('aliases', {}),\n",
        "            mappings=json_data['resources'].get('mappings', {}),\n",
        "            constraints=json_data['resources'].get('constraints', {})\n",
        "        )\n",
        "\n",
        "        ground_truth = None\n",
        "        if 'ground_truth' in json_data and json_data['ground_truth']:\n",
        "            ground_truth = [\n",
        "                ProtocolStep(id=step['id'], text=step['text'])\n",
        "                for step in json_data['ground_truth']['steps']\n",
        "            ]\n",
        "\n",
        "        return cls(\n",
        "            id=json_data['id'],\n",
        "            instruction=json_data['instruction'],\n",
        "            protocol=protocol_steps,\n",
        "            resources=resources,\n",
        "            ground_truth=ground_truth\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_yaml(cls, yaml_data: Dict) -> 'ExperimentTask':\n",
        "        \"\"\"YAMLãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ExperimentTaskã‚’ç”Ÿæˆ\"\"\"\n",
        "        return cls.from_json(yaml_data)\n",
        "\n",
        "print(\"âœ… ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rypmgB20Yc2",
        "outputId": "00b33c32-189d-4d35-af47-730ffa4cb127"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å®šç¾©ã—ã¾ã—ãŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆColabå¯¾å¿œï¼‰\n",
        "#@title 5. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å®Ÿè£… { display-mode: \"form\" }\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"å®Ÿé¨“ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ€ãƒ¼\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \"./data\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        if not self.data_dir.exists():\n",
        "            logger.warning(f\"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_dir}\")\n",
        "            self.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def check_data_availability(self) -> Dict[str, bool]:\n",
        "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        availability = {\n",
        "            'example_json': False,\n",
        "            'example_yaml': False,\n",
        "            'public': False,\n",
        "            'private': False\n",
        "        }\n",
        "\n",
        "        if (self.data_dir / \"example\" / \"json\").exists():\n",
        "            json_files = list((self.data_dir / \"example\" / \"json\").glob(\"task_*.json\"))\n",
        "            availability['example_json'] = len(json_files) > 0\n",
        "\n",
        "        if (self.data_dir / \"example\" / \"yaml\").exists():\n",
        "            yaml_files = list((self.data_dir / \"example\" / \"yaml\").glob(\"task_*.yaml\"))\n",
        "            availability['example_yaml'] = len(yaml_files) > 0\n",
        "\n",
        "        if (self.data_dir / \"public\").exists():\n",
        "            public_files = list((self.data_dir / \"public\").glob(\"pub_task_*.yaml\"))\n",
        "            availability['public'] = len(public_files) > 0\n",
        "\n",
        "        if (self.data_dir / \"private\").exists():\n",
        "            private_files = list((self.data_dir / \"private\").glob(\"pri_task_*.yaml\"))\n",
        "            availability['private'] = len(private_files) > 0\n",
        "\n",
        "        return availability\n",
        "\n",
        "    def load_task(self, file_path: str) -> ExperimentTask:\n",
        "        \"\"\"å˜ä¸€ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\"\"\"\n",
        "        file_path = Path(file_path)\n",
        "\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}\")\n",
        "\n",
        "        if file_path.suffix == '.json':\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            return ExperimentTask.from_json(data)\n",
        "        elif file_path.suffix == '.yaml':\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = yaml.safe_load(f)\n",
        "            return ExperimentTask.from_yaml(data)\n",
        "        else:\n",
        "            raise ValueError(f\"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_path.suffix}\")\n",
        "\n",
        "    def load_dataset(self, dataset_type: str = \"example\") -> List[ExperimentTask]:\n",
        "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®èª­ã¿è¾¼ã¿\"\"\"\n",
        "        tasks = []\n",
        "\n",
        "        if dataset_type == \"example\":\n",
        "            # JSONã¨YAMLã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "            json_dir = self.data_dir / \"example\" / \"json\"\n",
        "            yaml_dir = self.data_dir / \"example\" / \"yaml\"\n",
        "\n",
        "            # JSONå„ªå…ˆã§èª­ã¿è¾¼ã¿\n",
        "            if json_dir.exists():\n",
        "                for file_path in sorted(json_dir.glob(\"task_*.json\")):\n",
        "                    try:\n",
        "                        tasks.append(self.load_task(file_path))\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}\")\n",
        "            elif yaml_dir.exists():\n",
        "                for file_path in sorted(yaml_dir.glob(\"task_*.yaml\")):\n",
        "                    try:\n",
        "                        tasks.append(self.load_task(file_path))\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}\")\n",
        "\n",
        "        elif dataset_type == \"public\":\n",
        "            public_dir = self.data_dir / \"public\"\n",
        "            if public_dir.exists():\n",
        "                for file_path in sorted(public_dir.glob(\"pub_task_*.yaml\")):\n",
        "                    try:\n",
        "                        tasks.append(self.load_task(file_path))\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}\")\n",
        "\n",
        "        elif dataset_type == \"private\":\n",
        "            private_dir = self.data_dir / \"private\"\n",
        "            if private_dir.exists():\n",
        "                for file_path in sorted(private_dir.glob(\"pri_task_*.yaml\")):\n",
        "                    try:\n",
        "                        tasks.append(self.load_task(file_path))\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}\")\n",
        "\n",
        "        logger.info(f\"{dataset_type}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰{len(tasks)}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
        "        return tasks\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ€§ã®ãƒã‚§ãƒƒã‚¯\n",
        "loader = DataLoader()\n",
        "availability = loader.check_data_availability()\n",
        "print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ€§:\")\n",
        "for key, available in availability.items():\n",
        "    status = \"âœ…\" if available else \"âŒ\"\n",
        "    print(f\"  {status} {key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXF0Ou0B6sFB",
        "outputId": "2df67600-8350-4183-f5d2-62705c677b90"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ€§:\n",
            "  âŒ example_json\n",
            "  âŒ example_yaml\n",
            "  âŒ public\n",
            "  âŒ private\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰\n",
        "#@title 6. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ { display-mode: \"form\" }\n",
        "#@markdown ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™\n",
        "\n",
        "create_sample = True #@param {type:\"boolean\"}\n",
        "\n",
        "if create_sample and not any(availability.values()):\n",
        "    print(\"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...\")\n",
        "\n",
        "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
        "    sample_dir = Path(\"data/example/json\")\n",
        "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯1\n",
        "    sample_task_1 = {\n",
        "        \"id\": \"0001\",\n",
        "        \"instruction\": \"T75ãƒ•ãƒ©ã‚¹ã‚³ã«ç·šç¶­èŠ½ç´°èƒã‚’1æœ¬èµ·ã“ã—ã¦ãã ã•ã„\",\n",
        "        \"protocol\": {\n",
        "            \"steps\": [\n",
        "                {\"id\": 1, \"text\": \"15 mLé æ²ˆç®¡ã«9 mLã€Dishã«10 mLè¡€æ¸…å…¥ã‚ŠåŸ¹åœ°ã‚’å…¥ã‚Œã¦æ’æ¸©æ§½ã€ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã§æ¸©ã‚ã‚‹\"},\n",
        "                {\"id\": 2, \"text\": \"æ¶²ä½“çª’ç´ ã‚¿ãƒ³ã‚¯ã‹ã‚‰èµ·ã“ã™ç´°èƒã‚’ç´ æ—©ãå–ã‚Šå‡ºã™\"},\n",
        "                {\"id\": 3, \"text\": \"ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã®ã‚­ãƒ£ãƒƒãƒ—ã‚’å°‘ã—ç·©ã‚ã€å†åº¦é–‰ã‚ã‚‹\"},\n",
        "                {\"id\": 4, \"text\": \"æ’æ¸©æ§½ã§ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã‚’æ¸©ã‚ã€æ°·ã®å¡ŠãŒã‚„ã‚„æ®‹ã‚‹ãã‚‰ã„ã¾ã§æº¶ã‹ã™\"},\n",
        "                {\"id\": 5, \"text\": \"9 mLã®æ¸©ã‚ãŸåŸ¹åœ°ã‚’ä¸€éƒ¨ç§»ã—ã€æº¶ã‘ãŸéƒ¨åˆ†ã‹ã‚‰é æ²ˆç®¡ã«æˆ»ã™\"},\n",
        "                {\"id\": 6, \"text\": \"é æ²ˆç®¡ã®è“‹ã‚’ã—ã£ã‹ã‚Šé–‰ã‚ã€è»½ãè»¢å€’æ··å’Œã™ã‚‹\"},\n",
        "                {\"id\": 7, \"text\": \"1000 rpmã€5 minã€å®¤æ¸©ã§é å¿ƒã™ã‚‹\"},\n",
        "                {\"id\": 8, \"text\": \"ä¸Šæ¸…ã‚’å–ã‚Šé™¤ãã€Dishã§æ¸©ã‚ãŸåŸ¹åœ°ã§æ‡¸æ¿ã™ã‚‹\"},\n",
        "                {\"id\": 9, \"text\": \"ç´°èƒæ‡¸æ¿æ¶²ã‚’åŸ¹é¤Šå™¨ã«æ’­ç¨®ã™ã‚‹\"},\n",
        "                {\"id\": 10, \"text\": \"æ’­ç¨®ãƒ ãƒ©ãŒã§ããªã„ã‚ˆã†ã«åŸ¹é¤Šå™¨ã‚’æºã‚‰ã—ã€é™ã‹ã«ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã«å…¥ã‚Œã‚‹\"}\n",
        "            ]\n",
        "        },\n",
        "        \"resources\": {\n",
        "            \"aliases\": {\"Dish\": \"åŸ¹é¤Šå™¨\"},\n",
        "            \"mappings\": {\"Dish\": \"T75ãƒ•ãƒ©ã‚¹ã‚³\"},\n",
        "            \"constraints\": {\n",
        "                \"optional_steps\": [\n",
        "                    {\"name\": \"æ’æ¸©æ§½ã®é›»æºã‚’æ¶ˆã™\", \"earliest_index\": 7}\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        \"ground_truth\": {\n",
        "            \"steps\": [\n",
        "                {\"id\": 1, \"text\": \"æ’æ¸©æ§½ã®é›»æºã‚’å…¥ã‚Œã‚‹\"},\n",
        "                {\"id\": 2, \"text\": \"15 mLé æ²ˆç®¡ã«9 mLã€T75ãƒ•ãƒ©ã‚¹ã‚³ã«15 mLè¡€æ¸…å…¥ã‚ŠåŸ¹åœ°ã‚’å…¥ã‚Œã‚‹\"},\n",
        "                {\"id\": 3, \"text\": \"é æ²ˆç®¡ã¯æ’æ¸©æ§½ã€T75ãƒ•ãƒ©ã‚¹ã‚³ã¯ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã«å…¥ã‚Œã‚‹\"},\n",
        "                {\"id\": 4, \"text\": \"æ¶²ä½“çª’ç´ ã‚¿ãƒ³ã‚¯ã‹ã‚‰ç·šç¶­èŠ½ç´°èƒã®ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã‚’å–ã‚Šå‡ºã™\"},\n",
        "                {\"id\": 5, \"text\": \"ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã®ã‚­ãƒ£ãƒƒãƒ—ã‚’å°‘ã—ç·©ã‚ã€å†åº¦é–‰ã‚ã‚‹\"},\n",
        "                {\"id\": 6, \"text\": \"æ’æ¸©æ§½ã§ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã‚’æ¸©ã‚ã€æ°·ã®å¡ŠãŒã‚„ã‚„æ®‹ã‚‹ãã‚‰ã„ã¾ã§æº¶ã‹ã™\"},\n",
        "                {\"id\": 7, \"text\": \"é æ²ˆç®¡ã‹ã‚‰ã€9 mLã®æ¸©ã‚ãŸåŸ¹åœ°ã‚’ä¸€éƒ¨ç§»ã—ã€æº¶ã‘ãŸéƒ¨åˆ†ã‹ã‚‰é æ²ˆç®¡ã«æˆ»ã™\"},\n",
        "                {\"id\": 8, \"text\": \"ã‚¯ãƒ©ã‚¤ã‚ªãƒãƒ¥ãƒ¼ãƒ–ã«åŸ¹åœ°ã‚’åŠ ãˆã¦ãƒ”ãƒšãƒƒãƒ†ã‚£ãƒ³ã‚°ã—ã¦ã€ç´°èƒã‚’é æ²ˆç®¡ã«å›åã™ã‚‹\"},\n",
        "                {\"id\": 9, \"text\": \"é æ²ˆç®¡ã®è“‹ã‚’ã—ã£ã‹ã‚Šé–‰ã‚ã€è»½ãè»¢å€’æ··å’Œã™ã‚‹\"},\n",
        "                {\"id\": 10, \"text\": \"é æ²ˆç®¡ã‚’1000 rpmã€5 minã€å®¤æ¸©ã§é å¿ƒã™ã‚‹\"},\n",
        "                {\"id\": 11, \"text\": \"ä¸Šæ¸…ã‚’å–ã‚Šé™¤ãã€T75ãƒ•ãƒ©ã‚¹ã‚³ã§æ¸©ã‚ãŸåŸ¹åœ°ã§æ‡¸æ¿ã™ã‚‹\"},\n",
        "                {\"id\": 12, \"text\": \"ç´°èƒæ‡¸æ¿æ¶²ã‚’T75ãƒ•ãƒ©ã‚¹ã‚³ã«æˆ»ã™\"},\n",
        "                {\"id\": 13, \"text\": \"æ’­ç¨®ãƒ ãƒ©ãŒã§ããªã„ã‚ˆã†ã«T75ãƒ•ãƒ©ã‚¹ã‚³ã‚’æºã‚‰ã—ã€é™ã‹ã«ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã«å…¥ã‚Œã‚‹\"},\n",
        "                {\"id\": 14, \"text\": \"æ’æ¸©æ§½ã®é›»æºã‚’æ¶ˆã™\"}\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯2\n",
        "    sample_task_2 = {\n",
        "        \"id\": \"0002\",\n",
        "        \"instruction\": \"24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆã§ç´°èƒã®åŸ¹åœ°äº¤æ›ã‚’è¡Œã£ã¦ãã ã•ã„\",\n",
        "        \"protocol\": {\n",
        "            \"steps\": [\n",
        "                {\"id\": 1, \"text\": \"åŸ¹åœ°ã‚’æ’æ¸©æ§½ã§æ¸©ã‚ã‚‹\"},\n",
        "                {\"id\": 2, \"text\": \"ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰å–ã‚Šå‡ºã™\"},\n",
        "                {\"id\": 3, \"text\": \"å¤ã„åŸ¹åœ°ã‚’ã‚¢ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§é™¤å»ã™ã‚‹\"},\n",
        "                {\"id\": 4, \"text\": \"æ–°ã—ã„åŸ¹åœ°ã‚’å„ã‚¦ã‚§ãƒ«ã«åŠ ãˆã‚‹\"},\n",
        "                {\"id\": 5, \"text\": \"ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã«æˆ»ã™\"}\n",
        "            ]\n",
        "        },\n",
        "        \"resources\": {\n",
        "            \"aliases\": {\"ãƒ—ãƒ¬ãƒ¼ãƒˆ\": \"åŸ¹é¤Šãƒ—ãƒ¬ãƒ¼ãƒˆ\"},\n",
        "            \"mappings\": {\"ãƒ—ãƒ¬ãƒ¼ãƒˆ\": \"24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆ\", \"åŸ¹åœ°\": \"DMEMåŸ¹åœ°\"},\n",
        "            \"constraints\": {}\n",
        "        },\n",
        "        \"ground_truth\": {\n",
        "            \"steps\": [\n",
        "                {\"id\": 1, \"text\": \"DMEMåŸ¹åœ°ã‚’æ’æ¸©æ§½ã§37Â°Cã«æ¸©ã‚ã‚‹\"},\n",
        "                {\"id\": 2, \"text\": \"24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰å–ã‚Šå‡ºã™\"},\n",
        "                {\"id\": 3, \"text\": \"å¤ã„åŸ¹åœ°ã‚’ã‚¢ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§å„ã‚¦ã‚§ãƒ«ã‹ã‚‰æ…é‡ã«é™¤å»ã™ã‚‹\"},\n",
        "                {\"id\": 4, \"text\": \"æ–°ã—ã„DMEMåŸ¹åœ°ã‚’å„ã‚¦ã‚§ãƒ«ã«1 mLåŠ ãˆã‚‹\"},\n",
        "                {\"id\": 5, \"text\": \"24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã«æˆ»ã™\"}\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜\n",
        "    with open(sample_dir / \"task_0001.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(sample_task_1, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(sample_dir / \"task_0002.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(sample_task_2, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: {sample_dir}\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ€§ã‚’å†ãƒã‚§ãƒƒã‚¯\n",
        "    availability = loader.check_data_availability()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WclvJVKH6z4Y",
        "outputId": "466f0e0a-b8e8-482c-8044-c03814799835"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...\n",
            "âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: data/example/json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Pydanticãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ï¼ˆStructured Outputså¯¾å¿œï¼‰\n",
        "#@title 7. ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã®å®Ÿè£… { display-mode: \"form\" }\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "\n",
        "# Pydanticãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆStructured Outputsç”¨ï¼‰\n",
        "class ExperimentStepOutput(BaseModel):\n",
        "    \"\"\"å®Ÿé¨“æ‰‹é †ã®1ã‚¹ãƒ†ãƒƒãƒ—\"\"\"\n",
        "    id: int = Field(description=\"ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·\")\n",
        "    text: str = Field(description=\"å®Ÿé¨“æ‰‹é †ã®è©³ç´°ãªèª¬æ˜\")\n",
        "\n",
        "class ExperimentStepsOutput(BaseModel):\n",
        "    \"\"\"å®Ÿé¨“æ‰‹é †å…¨ä½“ã®å‡ºåŠ›\"\"\"\n",
        "    steps: List[ExperimentStepOutput] = Field(\n",
        "        description=\"å®Ÿé¨“æ‰‹é †ã®ãƒªã‚¹ãƒˆ\",\n",
        "        min_items=1\n",
        "    )\n",
        "\n",
        "    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
        "    total_steps: Optional[int] = Field(\n",
        "        default=None,\n",
        "        description=\"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°\"\n",
        "    )\n",
        "    estimated_time: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"æ¨å®šæ‰€è¦æ™‚é–“\"\n",
        "    )\n",
        "    safety_notes: Optional[List[str]] = Field(\n",
        "        default=None,\n",
        "        description=\"å®‰å…¨ä¸Šã®æ³¨æ„äº‹é …\"\n",
        "    )\n",
        "\n",
        "# ä»£æ›¿ã®ç°¡ç•¥ç‰ˆãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰\n",
        "class SimpleStepsOutput(BaseModel):\n",
        "    \"\"\"ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿé¨“æ‰‹é †ã®å‡ºåŠ›\"\"\"\n",
        "    steps: List[ExperimentStepOutput]\n",
        "\n",
        "class PromptBuilder:\n",
        "    \"\"\"å®Ÿé¨“æ‰‹é †ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã‚¯ãƒ©ã‚¹\"\"\"\n",
        "\n",
        "    def __init__(self, model_type: str = \"gpt-4o-mini\"):\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def build_system_prompt(self) -> str:\n",
        "        \"\"\"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰\"\"\"\n",
        "        return \"\"\"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã™ã€‚\n",
        "ä¸ãˆã‚‰ã‚ŒãŸå®Ÿé¨“æŒ‡ç¤ºã€ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‹ã‚‰ã€å®Ÿè¡Œå¯èƒ½ãªè©³ç´°ãªå®Ÿé¨“æ‰‹é †ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼š\n",
        "1. ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’åŸºã«ã€å…·ä½“çš„ãªå®Ÿé¨“æ‰‹é †ã‚’ä½œæˆã™ã‚‹\n",
        "2. ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆåˆ¥åï¼‰ã‚’é©åˆ‡ã«è§£æ±ºã—ã€æ­£ã—ã„å™¨å…·åãƒ»è©¦è–¬åã‚’ä½¿ç”¨ã™ã‚‹\n",
        "3. ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã«å¾“ã£ã¦ã€æ±ç”¨çš„ãªè¨˜è¿°ã‚’å…·ä½“çš„ãªå™¨å…·ãƒ»æ¡ä»¶ã«ç½®ãæ›ãˆã‚‹\n",
        "4. åˆ¶ç´„æ¡ä»¶ã‚’æº€ãŸã™ã‚ˆã†ã«æ‰‹é †ã‚’èª¿æ•´ã™ã‚‹\n",
        "5. å¿…è¦ã«å¿œã˜ã¦ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ ãƒ»ä¿®æ­£ãƒ»è©³ç´°åŒ–ã™ã‚‹\n",
        "6. æ¸©åº¦ã€æ™‚é–“ã€å›è»¢æ•°ãªã©ã®æ•°å€¤ã¯å…·ä½“çš„ã«è¨˜è¼‰ã™ã‚‹\n",
        "7. å®‰å…¨æ€§ã¨å®Ÿé¨“ã®æˆåŠŸã«å¿…è¦ãªæš—é»™çš„ãªæ‰‹é †ã‚‚å«ã‚ã‚‹\n",
        "8. å„ã‚¹ãƒ†ãƒƒãƒ—ã¯æ˜ç¢ºã§å®Ÿè¡Œå¯èƒ½ãªæŒ‡ç¤ºã¨ã—ã¦è¨˜è¿°ã™ã‚‹\"\"\"\n",
        "\n",
        "    def build_task_prompt(self, task: 'ExperimentTask') -> str:\n",
        "        \"\"\"ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰\"\"\"\n",
        "\n",
        "        prompt_parts = []\n",
        "\n",
        "        # å®Ÿé¨“æŒ‡ç¤º\n",
        "        prompt_parts.append(f\"ã€å®Ÿé¨“æŒ‡ç¤ºã€‘\\n{task.instruction}\\n\")\n",
        "\n",
        "        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«\n",
        "        prompt_parts.append(\"ã€å‚ç…§ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‘\")\n",
        "        for step in task.protocol:\n",
        "            prompt_parts.append(f\"{step.id}. {step.text}\")\n",
        "        prompt_parts.append(\"\")\n",
        "\n",
        "        # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±\n",
        "        if task.resources.aliases:\n",
        "            prompt_parts.append(\"ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆåˆ¥åï¼‰ã€‘\")\n",
        "            for key, value in task.resources.aliases.items():\n",
        "                prompt_parts.append(f\"- {key} = {value}\")\n",
        "            prompt_parts.append(\"\")\n",
        "\n",
        "        if task.resources.mappings:\n",
        "            prompt_parts.append(\"ã€ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå…·ä½“åŒ–ï¼‰ã€‘\")\n",
        "            for key, value in task.resources.mappings.items():\n",
        "                prompt_parts.append(f\"- {key} â†’ {value}\")\n",
        "            prompt_parts.append(\"\")\n",
        "\n",
        "        if task.resources.constraints:\n",
        "            prompt_parts.append(\"ã€åˆ¶ç´„æ¡ä»¶ã€‘\")\n",
        "            if 'optional_steps' in task.resources.constraints:\n",
        "                for opt_step in task.resources.constraints['optional_steps']:\n",
        "                    prompt_parts.append(\n",
        "                        f\"- ã€Œ{opt_step['name']}ã€ã‚’ã‚¹ãƒ†ãƒƒãƒ—{opt_step['earliest_index']}ä»¥é™ã«è¿½åŠ \"\n",
        "                    )\n",
        "            prompt_parts.append(\"\")\n",
        "\n",
        "        # ã‚¿ã‚¹ã‚¯æŒ‡ç¤º\n",
        "        prompt_parts.append(\"ã€ã‚¿ã‚¹ã‚¯ã€‘\")\n",
        "        prompt_parts.append(\"ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€å®Ÿè¡Œå¯èƒ½ãªè©³ç´°ãªå®Ÿé¨“æ‰‹é †ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
        "        prompt_parts.append(\"ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š\")\n",
        "        prompt_parts.append(\"1. ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦å…·ä½“çš„ãªå™¨å…·åã‚’ä½¿ç”¨\")\n",
        "        prompt_parts.append(\"2. å®Ÿé¨“æŒ‡ç¤ºã®è¦ä»¶ã‚’æº€ãŸã™\")\n",
        "        prompt_parts.append(\"3. å¿…è¦ãªæº–å‚™æ‰‹é †ï¼ˆä¾‹ï¼šæ©Ÿå™¨ã®é›»æºONï¼‰ã‚’è¿½åŠ \")\n",
        "        prompt_parts.append(\"4. å®‰å…¨æ€§ã¨æˆåŠŸã«å¿…è¦ãªè©³ç´°ã‚’å«ã‚ã‚‹\")\n",
        "        prompt_parts.append(\"5. åˆ¶ç´„æ¡ä»¶ã§æŒ‡å®šã•ã‚ŒãŸè¿½åŠ ã‚¹ãƒ†ãƒƒãƒ—ã‚’é©åˆ‡ãªä½ç½®ã«æŒ¿å…¥\")\n",
        "\n",
        "        return \"\\n\".join(prompt_parts)\n",
        "\n",
        "print(\"âœ… Pydanticãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\")\n",
        "print(f\"   - ExperimentStepsOutput: {len(ExperimentStepsOutput.model_fields)}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\")\n",
        "print(f\"   - SimpleStepsOutput: {len(SimpleStepsOutput.model_fields)}ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYc6bLUv7D5P",
        "outputId": "369fecd7-25c5-4d90-8aa3-f9d3beffbcaa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pydanticãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\n",
            "   - ExperimentStepsOutput: 4ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\n",
            "   - SimpleStepsOutput: 1ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: GPTå®Ÿé¨“æ‰‹é †ç”Ÿæˆå™¨ï¼ˆStructured Outputså°‚ç”¨ï¼‰\n",
        "#@title 8. GPTå®Ÿé¨“æ‰‹é †ç”Ÿæˆå™¨ï¼ˆStructured Outputsç‰ˆï¼‰ { display-mode: \"form\" }\n",
        "\n",
        "class GPTExperimentGenerator:\n",
        "    \"\"\"GPTã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“æ‰‹é †ç”Ÿæˆã‚¯ãƒ©ã‚¹ï¼ˆStructured Outputså°‚ç”¨ï¼‰\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: str = \"gpt-5-mini-2025-08-07\", temperature: float=1.0):\n",
        "        self.client = OpenAI(api_key=API_KEY)\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.prompt_builder = PromptBuilder(model_type=model)\n",
        "        self.max_retries = 3  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°\n",
        "\n",
        "    def generate_steps(self, task: 'ExperimentTask') -> List[Dict]:\n",
        "        \"\"\"Structured Outputsã‚’ä½¿ç”¨ã—ã¦å®Ÿé¨“æ‰‹é †ã‚’ç”Ÿæˆï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰\"\"\"\n",
        "\n",
        "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰\n",
        "        system_prompt = self.prompt_builder.build_system_prompt()\n",
        "        user_prompt = self.prompt_builder.build_task_prompt(task)\n",
        "\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                logger.info(f\"å®Ÿé¨“æ‰‹é †ç”Ÿæˆä¸­... (è©¦è¡Œ {attempt + 1}/{self.max_retries})\")\n",
        "\n",
        "                completion = self.client.chat.completions.parse(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_prompt}\n",
        "                    ],\n",
        "                    response_format=ExperimentStepsOutput,\n",
        "                    temperature=self.temperature\n",
        "                )\n",
        "\n",
        "                parsed_output = completion.choices[0].message.parsed\n",
        "\n",
        "                if parsed_output and parsed_output.steps:\n",
        "                    steps = [\n",
        "                        {\"id\": step.id, \"text\": step.text}\n",
        "                        for step in parsed_output.steps\n",
        "                    ]\n",
        "                    if parsed_output.total_steps:\n",
        "                        logger.info(f\"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {parsed_output.total_steps}\")\n",
        "                    if parsed_output.estimated_time:\n",
        "                        logger.info(f\"æ¨å®šæ™‚é–“: {parsed_output.estimated_time}\")\n",
        "                    if parsed_output.safety_notes:\n",
        "                        logger.info(f\"å®‰å…¨æ³¨æ„: {', '.join(parsed_output.safety_notes)}\")\n",
        "\n",
        "                    logger.info(f\"âœ… å®Ÿé¨“æ‰‹é †ç”ŸæˆæˆåŠŸ (è©¦è¡Œ {attempt + 1})\")\n",
        "                    return steps\n",
        "                else:\n",
        "                    logger.warning(f\"âš ï¸ ãƒ‘ãƒ¼ã‚¹å¤±æ•— (è©¦è¡Œ {attempt + 1}/{self.max_retries}): å‡ºåŠ›ãŒç©ºã§ã™\")\n",
        "                    last_error = \"Empty parsed output\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ (è©¦è¡Œ {attempt + 1}/{self.max_retries}): {str(e)}\")\n",
        "                last_error = e\n",
        "\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    logger.info(f\"â³ {wait_time}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...\")\n",
        "                    time.sleep(wait_time)\n",
        "\n",
        "        logger.error(f\"âŒ {self.max_retries}å›ã®è©¦è¡Œã™ã¹ã¦ã§å®Ÿé¨“æ‰‹é †ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "        logger.error(f\"æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_error}\")\n",
        "        self._log_generation_failure(task, last_error)\n",
        "\n",
        "        # ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆã€ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
        "        fallback_steps = [\n",
        "            {\"id\": step.id, \"text\": step.text}\n",
        "            for step in task.protocol\n",
        "        ]\n",
        "        logger.warning(f\"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ï¼ˆ{len(fallback_steps)}ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\")\n",
        "\n",
        "        return fallback_steps\n",
        "\n",
        "    def apply_resources(self, steps: List[Dict], task: 'ExperimentTask') -> List[Dict]:\n",
        "        \"\"\"ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’é©ç”¨ã—ã¦æ‰‹é †ã‚’ä¿®æ­£\"\"\"\n",
        "\n",
        "        modified_steps = deepcopy(steps)\n",
        "\n",
        "        for step in modified_steps:\n",
        "            text = step.get('text', '')\n",
        "\n",
        "            # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®é©ç”¨\n",
        "            for alias, actual in task.resources.aliases.items():\n",
        "                text = text.replace(alias, actual)\n",
        "\n",
        "            # ãƒãƒƒãƒ”ãƒ³ã‚°ã®é©ç”¨\n",
        "            for generic, specific in task.resources.mappings.items():\n",
        "                actual_generic = task.resources.aliases.get(generic, generic)\n",
        "                text = text.replace(actual_generic, specific)\n",
        "\n",
        "            step['text'] = text\n",
        "\n",
        "        if 'optional_steps' in task.resources.constraints:\n",
        "            for opt_step in task.resources.constraints['optional_steps']:\n",
        "                insert_pos = opt_step['earliest_index']\n",
        "                new_step = {\n",
        "                    'id': insert_pos,\n",
        "                    'text': opt_step['name']\n",
        "                }\n",
        "\n",
        "                # é©åˆ‡ãªä½ç½®ã«æŒ¿å…¥\n",
        "                if insert_pos <= len(modified_steps):\n",
        "                    modified_steps.insert(insert_pos - 1, new_step)\n",
        "                else:\n",
        "                    modified_steps.append(new_step)\n",
        "\n",
        "            # IDã®å†å‰²ã‚Šå½“ã¦\n",
        "            for i, step in enumerate(modified_steps, 1):\n",
        "                step['id'] = i\n",
        "\n",
        "        return modified_steps\n",
        "\n",
        "    def _log_generation_failure(self, task: 'ExperimentTask', error: Any):\n",
        "        \"\"\"ç”Ÿæˆå¤±æ•—ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²\"\"\"\n",
        "\n",
        "        log_dir = Path(\"./outputs/logs\")\n",
        "        log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_file = log_dir / f\"generation_failure_{task.id}_{timestamp}.json\"\n",
        "\n",
        "        log_data = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"task_id\": task.id,\n",
        "            \"instruction\": task.instruction,\n",
        "            \"model\": self.model,\n",
        "            \"error\": str(error),\n",
        "            \"protocol_steps\": len(task.protocol),\n",
        "            \"has_ground_truth\": task.ground_truth is not None\n",
        "        }\n",
        "\n",
        "        with open(log_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(log_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logger.info(f\"ğŸ“ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜: {log_file}\")\n",
        "\n",
        "print(\"âœ… Structured Outputså°‚ç”¨ã®GPTå®Ÿé¨“æ‰‹é †ç”Ÿæˆå™¨ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\")\n",
        "print(\"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼š\")\n",
        "print(\"   - ãƒ¢ãƒ‡ãƒ«: gpt-5-mini-2025-08-07\")\n",
        "print(\"   - æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°: 3\")\n",
        "print(\"   - ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°: ./outputs/logs/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXgbS-9P7jYt",
        "outputId": "af5f815e-894a-4916-f86b-f542aaac9169"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Structured Outputså°‚ç”¨ã®GPTå®Ÿé¨“æ‰‹é †ç”Ÿæˆå™¨ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\n",
            "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼š\n",
            "   - ãƒ¢ãƒ‡ãƒ«: gpt-5-mini-2025-08-07\n",
            "   - æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°: 3\n",
            "   - ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°: ./outputs/logs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ5ç‚¹æº€ç‚¹ç‰ˆï¼‰\n",
        "#@title 9. LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  { display-mode: \"form\" }\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "# å€‹åˆ¥è©•ä¾¡é …ç›®ã®çµæœ\n",
        "class CriterionScore(BaseModel):\n",
        "    \"\"\"å€‹åˆ¥è©•ä¾¡é …ç›®ã®ã‚¹ã‚³ã‚¢\"\"\"\n",
        "    name: str = Field(description=\"è©•ä¾¡é …ç›®å\")\n",
        "    score: int = Field(description=\"ã‚¹ã‚³ã‚¢ï¼ˆ1-5ï¼‰\", ge=1, le=5)\n",
        "    reasoning: str = Field(description=\"ã‚¹ã‚³ã‚¢ã®æ ¹æ‹ \")\n",
        "\n",
        "# æ¸›ç‚¹é …ç›®\n",
        "class Deduction(BaseModel):\n",
        "    \"\"\"æ¸›ç‚¹é …ç›®\"\"\"\n",
        "    reason: str = Field(description=\"æ¸›ç‚¹ç†ç”±\")\n",
        "    points: float = Field(description=\"æ¸›ç‚¹æ•°\", ge=0)\n",
        "    examples: List[str] = Field(default_factory=list, description=\"è©²å½“ç®‡æ‰€\")\n",
        "\n",
        "# è©•ä¾¡çµæœ\n",
        "class EvaluationResult(BaseModel):\n",
        "    \"\"\"å®Ÿé¨“æ‰‹é †ã®è©•ä¾¡çµæœ\"\"\"\n",
        "    task_id: str\n",
        "\n",
        "    # åŸºæœ¬è©•ä¾¡ï¼ˆå„5ç‚¹æº€ç‚¹ï¼‰\n",
        "    completeness: CriterionScore = Field(description=\"å¿…è¦ãªè¦ç´ ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹\")\n",
        "    specificity: CriterionScore = Field(description=\"æ•°å€¤ã‚„æ¡ä»¶ãŒæ˜ç¢ºã‹\")\n",
        "    consistency: CriterionScore = Field(description=\"çŸ›ç›¾ãŒãªãè«–ç†çš„ã‹\")\n",
        "    safety: CriterionScore = Field(description=\"å®‰å…¨ä¸Šã®é…æ…®ãŒé©åˆ‡ã‹\")\n",
        "    feasibility: CriterionScore = Field(description=\"å®Ÿéš›ã«å®Ÿè¡Œå¯èƒ½ã‹\")\n",
        "    clarity: CriterionScore = Field(description=\"ç†è§£ã—ã‚„ã™ãæ›–æ˜§ã•ãŒãªã„ã‹\")\n",
        "\n",
        "    # æ¸›ç‚¹\n",
        "    deductions: List[Deduction] = Field(default_factory=list)\n",
        "\n",
        "    # ç·åˆè©•ä¾¡\n",
        "    raw_score: float = Field(description=\"æ¸›ç‚¹å‰ã®å¹³å‡ã‚¹ã‚³ã‚¢\")\n",
        "    final_score: float = Field(description=\"æ¸›ç‚¹å¾Œã®æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆ1-5ï¼‰\")\n",
        "    feedback: str = Field(description=\"ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯\")\n",
        "    improvements: List[str] = Field(default_factory=list, description=\"æ”¹å–„ææ¡ˆ\")\n",
        "\n",
        "class LLMEvaluator:\n",
        "    \"\"\"LLMãƒ™ãƒ¼ã‚¹ã®å®Ÿé¨“æ‰‹é †è©•ä¾¡ã‚¯ãƒ©ã‚¹\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: str = \"gpt-5-mini-2025-08-07\",\n",
        "                 temperature: float = 1.0):\n",
        "        self.client = OpenAI(api_key=API_KEY)\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_retries = 2\n",
        "\n",
        "    def evaluate(self,\n",
        "                task: 'ExperimentTask',\n",
        "                generated_steps: List[Dict]) -> EvaluationResult:\n",
        "        \"\"\"å®Ÿé¨“æ‰‹é †ã‚’è©•ä¾¡\"\"\"\n",
        "\n",
        "        system_prompt = self._build_system_prompt()\n",
        "        user_prompt = self._build_user_prompt(task, generated_steps)\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                logger.info(f\"è©•ä¾¡å®Ÿè¡Œä¸­... (è©¦è¡Œ {attempt + 1}/{self.max_retries})\")\n",
        "\n",
        "                completion = self.client.chat.completions.parse(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_prompt}\n",
        "                    ],\n",
        "                    response_format=EvaluationResult,\n",
        "                    temperature=self.temperature\n",
        "                )\n",
        "\n",
        "                result = completion.choices[0].message.parsed\n",
        "                if result:\n",
        "                    result = self._calculate_final_score(result)\n",
        "                    logger.info(f\"âœ… è©•ä¾¡å®Œäº†: {result.final_score:.2f}/5.0\")\n",
        "                    return result\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"âš ï¸ è©•ä¾¡ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {str(e)}\")\n",
        "                if attempt < self.max_retries - 1:\n",
        "                    time.sleep(2)\n",
        "\n",
        "        logger.error(\"âŒ è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
        "        return self._create_default_evaluation(task.id, len(generated_steps))\n",
        "\n",
        "    def _build_system_prompt(self) -> str:\n",
        "        \"\"\"è©•ä¾¡ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\"\"\"\n",
        "        return \"\"\"ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã™ã€‚å®Ÿé¨“æ‰‹é †ã‚’ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã€æ¡ç‚¹åŸºæº–ï¼ˆ5ç‚¹æº€ç‚¹ï¼‰ã€‘\n",
        "1ç‚¹: èª¤ã£ã¦ã„ã‚‹ï¼è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ãªã„\n",
        "2ç‚¹: èª¤ã£ã¦ã„ã‚‹ãŒã€æ–¹å‘æ€§ã¯åˆã£ã¦ã„ã‚‹\n",
        "3ç‚¹: éƒ¨åˆ†çš„ã«æ­£ã—ã„\n",
        "4ç‚¹: æ­£ã—ã„ï¼è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹\n",
        "5ç‚¹: å„ªã‚Œã¦ã„ã‚‹ï¼éå¸¸ã«å®Ÿç”¨çš„\n",
        "\n",
        "ã€è©•ä¾¡é …ç›®ã€‘\n",
        "1. å®Œå…¨æ€§: å¿…è¦ãªå™¨å…·ã€è©¦è–¬ã€æ‰‹é †ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹\n",
        "2. å…·ä½“æ€§: æ¸©åº¦ã€æ™‚é–“ã€é‡ãªã©ãŒå…·ä½“çš„ãªæ•°å€¤ã§ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹\n",
        "3. ä¸€è²«æ€§: æ‰‹é †é–“ã§çŸ›ç›¾ãŒãªãè«–ç†çš„ã‹\n",
        "4. å®‰å…¨æ€§: å®‰å…¨ä¸Šã®é…æ…®ãŒé©åˆ‡ã‹\n",
        "5. å®Ÿè¡Œå¯èƒ½æ€§: å®Ÿéš›ã®ãƒ©ãƒœã§å®Ÿè¡Œå¯èƒ½ã‹\n",
        "6. æ˜ç¢ºæ€§: æŒ‡ç¤ºãŒæ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹\n",
        "\n",
        "ã€æ¸›ç‚¹é …ç›®ã€‘\n",
        "- ä¸è‡ªç„¶ãªæ—¥æœ¬èª: -1ç‚¹\n",
        "- äº‹å®Ÿã¨ç•°ãªã‚‹è¨˜è¿°: -1ç‚¹\n",
        "- éåº¦ãªå®‰å…¨æ€§: 2ç‚¹ã«å›ºå®š\n",
        "- å˜ä½ã®æ¬ è½: -0.5ç‚¹\n",
        "- æ›–æ˜§ãªè¡¨ç¾: -0.5ç‚¹\n",
        "\n",
        "å„é …ç›®ã‚’æ¡ç‚¹ã—ã€æ ¹æ‹ ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
        "æ¸›ç‚¹é …ç›®ãŒã‚ã‚Œã°å…·ä½“ä¾‹ã¨ã¨ã‚‚ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\"\"\"\n",
        "\n",
        "    def _build_user_prompt(self,\n",
        "                          task: 'ExperimentTask',\n",
        "                          generated_steps: List[Dict]) -> str:\n",
        "        \"\"\"è©•ä¾¡ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\"\"\"\n",
        "\n",
        "        prompt_parts = []\n",
        "\n",
        "        prompt_parts.append(f\"ã€ã‚¿ã‚¹ã‚¯ã€‘{task.instruction}\\n\")\n",
        "\n",
        "        prompt_parts.append(\"ã€ç”Ÿæˆã•ã‚ŒãŸæ‰‹é †ã€‘\")\n",
        "        for step in generated_steps:\n",
        "            prompt_parts.append(f\"{step['id']}. {step['text']}\")\n",
        "        prompt_parts.append(\"\")\n",
        "\n",
        "        prompt_parts.append(\"ã€å…ƒã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‘\")\n",
        "        for step in task.protocol:\n",
        "            prompt_parts.append(f\"{step.id}. {step.text}\")\n",
        "        prompt_parts.append(\"\")\n",
        "\n",
        "        if task.resources.aliases or task.resources.mappings:\n",
        "            prompt_parts.append(\"ã€ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã€‘\")\n",
        "            if task.resources.aliases:\n",
        "                for k, v in task.resources.aliases.items():\n",
        "                    prompt_parts.append(f\"  {k} = {v}\")\n",
        "            if task.resources.mappings:\n",
        "                for k, v in task.resources.mappings.items():\n",
        "                    prompt_parts.append(f\"  {k} â†’ {v}\")\n",
        "            prompt_parts.append(\"\")\n",
        "\n",
        "        prompt_parts.append(\"ä¸Šè¨˜ã®å®Ÿé¨“æ‰‹é †ã‚’6ã¤ã®é …ç›®ã§è©•ä¾¡ã—ã€æ¸›ç‚¹é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "        return \"\\n\".join(prompt_parts)\n",
        "\n",
        "    def _calculate_final_score(self, result: EvaluationResult) -> EvaluationResult:\n",
        "        \"\"\"æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\"\"\"\n",
        "\n",
        "        # åŸºæœ¬ã‚¹ã‚³ã‚¢ã®å¹³å‡\n",
        "        scores = [\n",
        "            result.completeness.score,\n",
        "            result.specificity.score,\n",
        "            result.consistency.score,\n",
        "            result.safety.score,\n",
        "            result.feasibility.score,\n",
        "            result.clarity.score\n",
        "        ]\n",
        "        result.raw_score = sum(scores) / len(scores)\n",
        "\n",
        "        # æ¸›ç‚¹é©ç”¨\n",
        "        total_deduction = 0\n",
        "        for deduction in result.deductions:\n",
        "            if \"éåº¦ãªå®‰å…¨æ€§\" in deduction.reason:\n",
        "                result.final_score = 2.0\n",
        "                return result\n",
        "            total_deduction += deduction.points\n",
        "\n",
        "        result.final_score = max(1.0, result.raw_score - total_deduction)\n",
        "        return result\n",
        "\n",
        "    def _create_default_evaluation(self, task_id: str, num_steps: int) -> EvaluationResult:\n",
        "        \"\"\"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè©•ä¾¡\"\"\"\n",
        "\n",
        "        default = CriterionScore(\n",
        "            name=\"default\",\n",
        "            score=3,\n",
        "            reasoning=\"è©•ä¾¡å¤±æ•—ã®ãŸã‚ä¸­é–“å€¤ã‚’ä½¿ç”¨\"\n",
        "        )\n",
        "\n",
        "        return EvaluationResult(\n",
        "            task_id=task_id,\n",
        "            completeness=default,\n",
        "            specificity=default,\n",
        "            consistency=default,\n",
        "            safety=default,\n",
        "            feasibility=default,\n",
        "            clarity=default,\n",
        "            deductions=[],\n",
        "            raw_score=3.0,\n",
        "            final_score=3.0,\n",
        "            feedback=\"è©•ä¾¡ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸ\",\n",
        "            improvements=[]\n",
        "        )\n",
        "\n",
        "    def create_summary(self, results: List[EvaluationResult]) -> pd.DataFrame:\n",
        "        \"\"\"è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆ\"\"\"\n",
        "\n",
        "        data = []\n",
        "        for r in results:\n",
        "            data.append({\n",
        "                'task_id': r.task_id,\n",
        "                'completeness': r.completeness.score,\n",
        "                'specificity': r.specificity.score,\n",
        "                'consistency': r.consistency.score,\n",
        "                'safety': r.safety.score,\n",
        "                'feasibility': r.feasibility.score,\n",
        "                'clarity': r.clarity.score,\n",
        "                'deductions': len(r.deductions),\n",
        "                'final_score': r.final_score\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "print(\"âœ… è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\")\n",
        "print(\"  - 5ç‚¹æº€ç‚¹è©•ä¾¡\")\n",
        "print(\"  - 6ã¤ã®è©•ä¾¡é …ç›®\")\n",
        "print(\"  - è‡ªå‹•æ¸›ç‚¹æ©Ÿèƒ½\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifjqbRoeFXgt",
        "outputId": "bce58e2a-6ea4-4a72-ca1e-436d55bb5683"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\n",
            "  - 5ç‚¹æº€ç‚¹è©•ä¾¡\n",
            "  - 6ã¤ã®è©•ä¾¡é …ç›®\n",
            "  - è‡ªå‹•æ¸›ç‚¹æ©Ÿèƒ½\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ çµ±åˆç‰ˆï¼‰\n",
        "#@title 10. ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ { display-mode: \"form\" }\n",
        "\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "\n",
        "class RunManager:\n",
        "    \"\"\"å®Ÿè¡Œã”ã¨ã®çµæœç®¡ç†ã‚¯ãƒ©ã‚¹\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str = \"./outputs\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.runs_dir = self.base_dir / \"runs\"\n",
        "        self.runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # å®Ÿè¡ŒIDã®ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + çŸ­ã„UUIDï¼‰\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        short_id = str(uuid.uuid4())[:4]\n",
        "        self.run_id = f\"{timestamp}_{short_id}\"\n",
        "\n",
        "        self.run_dir = self.runs_dir / self.run_id\n",
        "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        (self.run_dir / \"evaluations\").mkdir(exist_ok=True)\n",
        "        (self.run_dir / \"logs\").mkdir(exist_ok=True)\n",
        "\n",
        "        logger.info(f\"ğŸ“ å®Ÿè¡ŒID: {self.run_id}\")\n",
        "        logger.info(f\"ğŸ“ çµæœä¿å­˜å…ˆ: {self.run_dir}\")\n",
        "\n",
        "        # å®Ÿè¡Œå±¥æ­´ã®æ›´æ–°\n",
        "        self._update_run_history()\n",
        "\n",
        "    def save_config(self, config: Dict):\n",
        "        config_file = self.run_dir / \"config.json\"\n",
        "        config_data = {\n",
        "            \"run_id\": self.run_id,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            **config\n",
        "        }\n",
        "        with open(config_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(config_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def get_path(self, filename: str) -> Path:\n",
        "        return self.run_dir / filename\n",
        "\n",
        "    def update_latest(self):\n",
        "        \"\"\"æœ€æ–°å®Ÿè¡Œãƒ•ã‚©ãƒ«ãƒ€ã‚’æ›´æ–°\"\"\"\n",
        "        latest_dir = self.base_dir / \"latest\"\n",
        "\n",
        "        # æ—¢å­˜ã®latestãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤\n",
        "        if latest_dir.exists():\n",
        "            shutil.rmtree(latest_dir)\n",
        "\n",
        "        # ç¾åœ¨ã®å®Ÿè¡Œå†…å®¹ã‚’latestã«ã‚³ãƒ”ãƒ¼\n",
        "        shutil.copytree(self.run_dir, latest_dir)\n",
        "\n",
        "        logger.info(f\"ğŸ“ æœ€æ–°çµæœã‚’æ›´æ–°: {latest_dir}\")\n",
        "\n",
        "    def _update_run_history(self):\n",
        "        \"\"\"å®Ÿè¡Œå±¥æ­´ã‚’æ›´æ–°\"\"\"\n",
        "        history_file = self.base_dir / \"run_history.json\"\n",
        "\n",
        "        # æ—¢å­˜ã®å±¥æ­´ã‚’èª­ã¿è¾¼ã¿\n",
        "        if history_file.exists():\n",
        "            with open(history_file, 'r', encoding='utf-8') as f:\n",
        "                history = json.load(f)\n",
        "        else:\n",
        "            history = {\"runs\": []}\n",
        "\n",
        "        # æ–°ã—ã„å®Ÿè¡Œã‚’è¿½åŠ \n",
        "        history[\"runs\"].append({\n",
        "            \"run_id\": self.run_id,\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"status\": \"running\"\n",
        "        })\n",
        "\n",
        "        # ä¿å­˜\n",
        "        with open(history_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(history, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def finalize(self, status: str = \"completed\", summary: Dict = None):\n",
        "        \"\"\"å®Ÿè¡Œã‚’å®Œäº†\"\"\"\n",
        "        history_file = self.base_dir / \"run_history.json\"\n",
        "\n",
        "        if history_file.exists():\n",
        "            with open(history_file, 'r', encoding='utf-8') as f:\n",
        "                history = json.load(f)\n",
        "\n",
        "            # è©²å½“ã™ã‚‹å®Ÿè¡Œã‚’æ›´æ–°\n",
        "            for run in history[\"runs\"]:\n",
        "                if run[\"run_id\"] == self.run_id:\n",
        "                    run[\"status\"] = status\n",
        "                    run[\"end_time\"] = datetime.now().isoformat()\n",
        "                    if summary:\n",
        "                        run[\"summary\"] = summary\n",
        "                    break\n",
        "\n",
        "            with open(history_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(history, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # æœ€æ–°ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ›´æ–°\n",
        "        self.update_latest()\n",
        "\n",
        "        logger.info(f\"âœ… å®Ÿè¡Œå®Œäº†: {self.run_id} ({status})\")\n",
        "\n",
        "\n",
        "class ExperimentPipeline:\n",
        "    \"\"\"å®Ÿé¨“æ‰‹é †ç”Ÿæˆã¨è©•ä¾¡ã®ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-5-mini-2025-08-07\", temperature: float= 1.0):\n",
        "        if not API_KEY:\n",
        "            raise ValueError(\"APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "        self.gpt_generator = GPTExperimentGenerator(model=model, temperature=temperature)\n",
        "        self.llm_evaluator = LLMEvaluator(model=model, temperature=temperature)\n",
        "        self.data_loader = DataLoader()\n",
        "        self.run_manager = RunManager()\n",
        "\n",
        "        # è¨­å®šã‚’ä¿å­˜\n",
        "        self.run_manager.save_config({\n",
        "            \"model\": model,\n",
        "            \"temperature\": temperature,\n",
        "            \"generator\": \"structured_outputs\",\n",
        "            \"evaluator\": \"llm_5point\"\n",
        "        })\n",
        "\n",
        "        logger.info(f\"âœ… Pipeline initialized with model: {model}, temperature: {temperature}\")\n",
        "\n",
        "    def process_task(self, task: 'ExperimentTask') -> Dict:\n",
        "        \"\"\"å˜ä¸€ã‚¿ã‚¹ã‚¯ã®å‡¦ç†ï¼ˆç”Ÿæˆï¼‹è©•ä¾¡ï¼‰\"\"\"\n",
        "\n",
        "        logger.info(f\"ğŸ”¬ Processing task {task.id}: {task.instruction[:50]}...\")\n",
        "        # æ‰‹é †ç”Ÿæˆ\n",
        "        generated_steps = self.gpt_generator.generate_steps(task)\n",
        "        # ãƒªã‚½ãƒ¼ã‚¹é©ç”¨\n",
        "        generated_steps = self.gpt_generator.apply_resources(generated_steps, task)\n",
        "        # LLMè©•ä¾¡\n",
        "        evaluation = self.llm_evaluator.evaluate(task, generated_steps)\n",
        "\n",
        "        logger.info(f\"ğŸ“Š è©•ä¾¡ã‚¹ã‚³ã‚¢: {evaluation.final_score:.2f}/5.0\")\n",
        "\n",
        "        result = {\n",
        "            'task_id': task.id,\n",
        "            'instruction': task.instruction,\n",
        "            'generated_steps': generated_steps,\n",
        "            'evaluation': evaluation,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_dataset(self,\n",
        "                       dataset_type: str = \"example\",\n",
        "                       save_results: bool = True) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®å‡¦ç†\"\"\"\n",
        "\n",
        "        tasks = self.data_loader.load_dataset(dataset_type)\n",
        "        if not tasks:\n",
        "            logger.error(f\"âŒ No tasks found in {dataset_type} dataset\")\n",
        "            self.run_manager.finalize(\n",
        "                status=\"failed\",\n",
        "                summary={\"error\": \"No tasks found\"})\n",
        "            return pd.DataFrame(), []\n",
        "\n",
        "        logger.info(f\"ğŸ“š Processing {len(tasks)} tasks from {dataset_type} dataset\")\n",
        "\n",
        "        self.run_manager.save_config({\n",
        "            \"dataset_type\": dataset_type,\n",
        "            \"num_tasks\": len(tasks)\n",
        "        })\n",
        "\n",
        "        results = []\n",
        "        evaluations = []\n",
        "\n",
        "        # å„ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†\n",
        "        for task in tqdm(tasks, desc=f\"Processing {dataset_type} dataset\"):\n",
        "            try:\n",
        "                result = self.process_task(task)\n",
        "                results.append(result)\n",
        "                evaluations.append(result['evaluation'])\n",
        "\n",
        "                # å€‹åˆ¥çµæœã‚’å³åº§ã«ä¿å­˜\n",
        "                if save_results:\n",
        "                    eval_file = self.run_manager.get_path(f\"evaluations/{task.id}.json\")\n",
        "                    with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump({\n",
        "                            'task_id': result['task_id'],\n",
        "                            'instruction': result['instruction'],\n",
        "                            'num_steps': len(result['generated_steps']),\n",
        "                            'evaluation': result['evaluation'].model_dump()\n",
        "                        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Task {task.id} failed: {e}\")\n",
        "                # ã‚¨ãƒ©ãƒ¼ã‚‚ãƒ­ã‚°ã«è¨˜éŒ²\n",
        "                error_file = self.run_manager.get_path(f\"logs/error_{task.id}.txt\")\n",
        "                with open(error_file, 'w') as f:\n",
        "                    f.write(str(e))\n",
        "\n",
        "            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–\n",
        "            time.sleep(1)\n",
        "\n",
        "        # çµæœã®ä¿å­˜\n",
        "        if save_results:\n",
        "            self._save_results(results, dataset_type)\n",
        "\n",
        "        # è©•ä¾¡ã‚µãƒãƒªãƒ¼ã®ä½œæˆ\n",
        "        summary_df = self.llm_evaluator.create_summary(evaluations)\n",
        "\n",
        "        # ã‚µãƒãƒªãƒ¼ã‚’CSVã§ä¿å­˜\n",
        "        if not summary_df.empty:\n",
        "            summary_file = self.run_manager.get_path(\"summary.csv\")\n",
        "            summary_df.to_csv(summary_file, index=False)\n",
        "\n",
        "            # å®Ÿè¡Œå®Œäº†\n",
        "            self.run_manager.finalize(status=\"completed\", summary={\n",
        "                \"num_tasks\": len(tasks),\n",
        "                \"num_completed\": len(results),\n",
        "                \"avg_score\": summary_df['final_score'].mean(),\n",
        "                \"dataset_type\": dataset_type\n",
        "            })\n",
        "\n",
        "        return summary_df, results\n",
        "\n",
        "    def _save_results(self, results: List[Dict], dataset_type: str):\n",
        "        \"\"\"çµæœã®ä¿å­˜\"\"\"\n",
        "\n",
        "        # ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰æå‡ºå½¢å¼\n",
        "        submission = {\n",
        "            'team_name': 'baseline',\n",
        "            'submission_time': datetime.now().isoformat(),\n",
        "            'dataset_type': dataset_type,\n",
        "            'model': self.gpt_generator.model,\n",
        "            'predictions': []\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            submission['predictions'].append({\n",
        "                'task_id': result['task_id'],\n",
        "                'steps': result['generated_steps']\n",
        "            })\n",
        "\n",
        "        submission_file = self.run_manager.get_path(\"submission.json\")\n",
        "        with open(submission_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(submission, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logger.info(f\"ğŸ’¾ Submission saved to {submission_file}\")\n",
        "\n",
        "print(\"âœ… å®Ÿè¡Œç®¡ç†æ©Ÿèƒ½ä»˜ããƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\")\n",
        "print(\"ç‰¹å¾´:\")\n",
        "print(\"  - å®Ÿè¡Œã”ã¨ã«ä¸€æ„ã®ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆYYYYMMDD_HHMMSS_IDï¼‰\")\n",
        "print(\"  - å®Ÿè¡Œè¨­å®šã®è‡ªå‹•ä¿å­˜\")\n",
        "print(\"  - æœ€æ–°å®Ÿè¡Œã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆlatest/ï¼‰\")\n",
        "print(\"  - å®Ÿè¡Œå±¥æ­´ã®ç®¡ç†ï¼ˆrun_history.jsonï¼‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efdCpfUVFiKn",
        "outputId": "bd6ed009-71f8-4d3d-8d2a-a2daefd11b0c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… å®Ÿè¡Œç®¡ç†æ©Ÿèƒ½ä»˜ããƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã—ãŸ\n",
            "ç‰¹å¾´:\n",
            "  - å®Ÿè¡Œã”ã¨ã«ä¸€æ„ã®ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆYYYYMMDD_HHMMSS_IDï¼‰\n",
            "  - å®Ÿè¡Œè¨­å®šã®è‡ªå‹•ä¿å­˜\n",
            "  - æœ€æ–°å®Ÿè¡Œã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆlatest/ï¼‰\n",
            "  - å®Ÿè¡Œå±¥æ­´ã®ç®¡ç†ï¼ˆrun_history.jsonï¼‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: å®Ÿè¡Œ\n",
        "#@title 11. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ { run: \"auto\" }\n",
        "#@markdown ä»¥ä¸‹ã®è¨­å®šã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã‚’å®Ÿè¡Œã—ã¾ã™\n",
        "\n",
        "dataset_type = \"example\" #@param [\"example\", \"public\", \"private\"]\n",
        "save_results = True #@param {type:\"boolean\"}\n",
        "model_name = \"gpt-5-mini-2025-08-07\" #@param [\"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-08-06\", \"gpt-5-2025-08-07\", \"gpt-5-mini-2025-08-07\", \"gpt-5-nano-2025-08-07\"]\n",
        "#@markdown gpt-4o-mini, gpt-4o-2024-08-06, ã‚ã‚‹ã„ã¯ãã‚Œä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚<br>\n",
        "#@markdown (Structured outputã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚) <br>\n",
        "#@markdown gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€temperature=1.0ã¨ã—ã¦ãã ã•ã„ã€‚\n",
        "temperature = 1.0 #@param\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
        "    print(\"Cell 2ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„\")\n",
        "\n",
        "else:\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸš€ LA-Bench 2025 Baseline Execution\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Dataset: {dataset_type}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Temperature: {temperature}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–ã¨å®Ÿè¡Œ\n",
        "    pipeline = ExperimentPipeline(model=model_name, temperature=temperature)\n",
        "    summary_df, results = pipeline.process_dataset(\n",
        "        dataset_type=dataset_type,\n",
        "        save_results=save_results\n",
        "    )\n",
        "\n",
        "    # çµæœã®è¡¨ç¤º\n",
        "    if not summary_df.empty:\n",
        "        print(\"\\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ï¼ˆ5ç‚¹æº€ç‚¹ï¼‰\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # å„ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡\n",
        "        print(\"\\nã€ã‚¿ã‚¹ã‚¯åˆ¥è©•ä¾¡ã€‘\")\n",
        "        display_cols = ['task_id', 'final_score', 'completeness', 'specificity',\n",
        "                        'consistency', 'safety', 'feasibility', 'clarity']\n",
        "        print(summary_df[display_cols].to_string(index=False))\n",
        "\n",
        "        # çµ±è¨ˆæƒ…å ±\n",
        "        print(\"\\nã€çµ±è¨ˆæƒ…å ±ã€‘\")\n",
        "        print(f\"ã‚¿ã‚¹ã‚¯æ•°: {len(summary_df)}\")\n",
        "        print(f\"å¹³å‡ã‚¹ã‚³ã‚¢: {summary_df['final_score'].mean():.2f}/5.0\")\n",
        "        print(f\"æœ€é«˜ã‚¹ã‚³ã‚¢: {summary_df['final_score'].max():.2f}/5.0\")\n",
        "        print(f\"æœ€ä½ã‚¹ã‚³ã‚¢: {summary_df['final_score'].min():.2f}/5.0\")\n",
        "        print(f\"æ¨™æº–åå·®: {summary_df['final_score'].std():.2f}\")\n",
        "\n",
        "        # è©•ä¾¡é …ç›®åˆ¥ã®å¹³å‡\n",
        "        print(\"\\nã€è©•ä¾¡é …ç›®åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢ã€‘\")\n",
        "        criteria = ['completeness', 'specificity', 'consistency',\n",
        "                    'safety', 'feasibility', 'clarity']\n",
        "        for criterion in criteria:\n",
        "            avg_score = summary_df[criterion].mean()\n",
        "            print(f\"  {criterion:12s}: {avg_score:.2f}/5.0\")\n",
        "\n",
        "        # æ¸›ç‚¹ã®çµ±è¨ˆ\n",
        "        if 'deductions' in summary_df.columns:\n",
        "            total_deductions = summary_df['deductions'].sum()\n",
        "            if total_deductions > 0:\n",
        "                print(f\"\\nã€æ¸›ç‚¹çµ±è¨ˆã€‘\")\n",
        "                print(f\"  ç·æ¸›ç‚¹é …ç›®æ•°: {total_deductions}\")\n",
        "                print(f\"  æ¸›ç‚¹ãŒã‚ã£ãŸã‚¿ã‚¹ã‚¯: {(summary_df['deductions'] > 0).sum()}/{len(summary_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "b40ac992b90b4aaa88a76bfdfbdac9c2",
            "97a08cebeac54ab09f411be9aba92df7",
            "44cf68d6ead14da0aa7728d2dcd62dea",
            "04780b69229e4843b6afe0749eea65f0",
            "2f63e788f9cb4cee8c5f7a1bf3493dc2",
            "c82338319c7243c2b140cab742e6cbf6",
            "f35d254373ad43c9b9286d6f09c60cce",
            "59189fe39e3d4138ac6b2827be7c62e6",
            "cb000000b4984fba9929e157d7325495",
            "56626730a9a546908e25dee10e500ba3",
            "ef88243320c44924ac5e804ffee3069b"
          ]
        },
        "cellView": "form",
        "id": "tdc_5sgoKtPi",
        "outputId": "6cedb83b-cadf-4b4b-b51a-4921a10e73d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸš€ LA-Bench 2025 Baseline Execution\n",
            "============================================================\n",
            "Dataset: example\n",
            "Model: gpt-5-mini-2025-08-07\n",
            "Temperature: 1.0\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing example dataset:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b40ac992b90b4aaa88a76bfdfbdac9c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ï¼ˆ5ç‚¹æº€ç‚¹ï¼‰\n",
            "============================================================\n",
            "\n",
            "ã€ã‚¿ã‚¹ã‚¯åˆ¥è©•ä¾¡ã€‘\n",
            "                  task_id  final_score  completeness  specificity  consistency  safety  feasibility  clarity\n",
            "T75_fibroblast_thawing_v1     2.000000             4            3            3       4            4        3\n",
            "                 task_001     2.333333             4            4            3       5            4        3\n",
            "\n",
            "ã€çµ±è¨ˆæƒ…å ±ã€‘\n",
            "ã‚¿ã‚¹ã‚¯æ•°: 2\n",
            "å¹³å‡ã‚¹ã‚³ã‚¢: 2.17/5.0\n",
            "æœ€é«˜ã‚¹ã‚³ã‚¢: 2.33/5.0\n",
            "æœ€ä½ã‚¹ã‚³ã‚¢: 2.00/5.0\n",
            "æ¨™æº–åå·®: 0.24\n",
            "\n",
            "ã€è©•ä¾¡é …ç›®åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢ã€‘\n",
            "  completeness: 4.00/5.0\n",
            "  specificity : 3.50/5.0\n",
            "  consistency : 3.00/5.0\n",
            "  safety      : 4.50/5.0\n",
            "  feasibility : 4.00/5.0\n",
            "  clarity     : 3.00/5.0\n",
            "\n",
            "ã€æ¸›ç‚¹çµ±è¨ˆã€‘\n",
            "  ç·æ¸›ç‚¹é …ç›®æ•°: 4\n",
            "  æ¸›ç‚¹ãŒã‚ã£ãŸã‚¿ã‚¹ã‚¯: 2/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: çµæœã®å¯è¦–åŒ–ã¨åˆ†æ\n",
        "#@title 12. çµæœã®å¯è¦–åŒ–ã¨åˆ†æ { display-mode: \"form\" }\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "if 'summary_df' in locals() and not summary_df.empty:\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ“Š LA-Bench 2025 è©•ä¾¡çµæœåˆ†æ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
        "    print(\"\\nã€å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘\")\n",
        "    mean_score = summary_df['final_score'].mean()\n",
        "    std_score = summary_df['final_score'].std()\n",
        "    ci_95 = 1.96 * std_score / np.sqrt(len(summary_df))\n",
        "\n",
        "    print(f\"  æœ€çµ‚ã‚¹ã‚³ã‚¢: {mean_score:.2f} Â± {ci_95:.2f} (95% CI)\")\n",
        "    print(f\"  ä¸­å¤®å€¤: {summary_df['final_score'].median():.2f}\")\n",
        "    print(f\"  å››åˆ†ä½ç¯„å›²: [{summary_df['final_score'].quantile(0.25):.2f}, {summary_df['final_score'].quantile(0.75):.2f}]\")\n",
        "\n",
        "    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¤å®š\n",
        "    if mean_score >= 4.0:\n",
        "        grade = \"ğŸ† å„ªç§€\"\n",
        "    elif mean_score >= 3.5:\n",
        "        grade = \"âœ… è‰¯å¥½\"\n",
        "    elif mean_score >= 3.0:\n",
        "        grade = \"ğŸ“ˆ æ”¹å–„ä½™åœ°ã‚ã‚Š\"\n",
        "    else:\n",
        "        grade = \"âš ï¸ è¦æ”¹å–„\"\n",
        "    print(f\"  åˆ¤å®š: {grade}\")\n",
        "\n",
        "    # 2. ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®è©³ç´°åˆ†æ\n",
        "    print(\"\\nã€ã‚¹ã‚³ã‚¢åˆ†å¸ƒã€‘\")\n",
        "    hist_data = []\n",
        "    for i in range(1, 6):\n",
        "        if i == 5:\n",
        "            count = (summary_df['final_score'] == i).sum()\n",
        "        else:\n",
        "            count = ((summary_df['final_score'] >= i) & (summary_df['final_score'] < i+1)).sum()\n",
        "        pct = count / len(summary_df) * 100\n",
        "        bar = \"â–ˆ\" * int(pct / 2)\n",
        "        hist_data.append(f\"  [{i}]: {count:3d} ({pct:5.1f}%) {bar}\")\n",
        "    for line in hist_data:\n",
        "        print(line)\n",
        "\n",
        "    # 3. è©•ä¾¡é …ç›®ã®å¼·ã¿ãƒ»å¼±ã¿åˆ†æ\n",
        "    criteria = ['completeness', 'specificity', 'consistency',\n",
        "               'safety', 'feasibility', 'clarity']\n",
        "    available = [c for c in criteria if c in summary_df.columns]\n",
        "\n",
        "    if available:\n",
        "        print(\"\\nã€è©•ä¾¡é …ç›®åˆ†æã€‘\")\n",
        "        criteria_stats = []\n",
        "        for c in available:\n",
        "            mean = summary_df[c].mean()\n",
        "            std = summary_df[c].std()\n",
        "            criteria_stats.append((c, mean, std))\n",
        "\n",
        "        # ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º\n",
        "        criteria_stats.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"  å¼·ã¿ï¼ˆä¸Šä½3é …ç›®ï¼‰:\")\n",
        "        for c, mean, std in criteria_stats[:3]:\n",
        "            indicator = \"â—†\" * int(mean)\n",
        "            print(f\"    {c:12s}: {mean:.2f}Â±{std:.2f} {indicator}\")\n",
        "\n",
        "        print(\"  å¼±ã¿ï¼ˆä¸‹ä½3é …ç›®ï¼‰:\")\n",
        "        for c, mean, std in criteria_stats[-3:]:\n",
        "            indicator = \"â—‡\" * int(5 - mean)\n",
        "            print(f\"    {c:12s}: {mean:.2f}Â±{std:.2f} {indicator}\")\n",
        "\n",
        "        # æ”¹å–„å„ªå…ˆåº¦ï¼ˆæ¨™æº–åå·®ãŒå¤§ããå¹³å‡ãŒä½ã„ï¼‰\n",
        "        priority = sorted(criteria_stats, key=lambda x: -x[2] * (5 - x[1]))\n",
        "        print(f\"  æœ€å„ªå…ˆæ”¹å–„é …ç›®: {priority[0][0]} (ä¸å®‰å®šã‹ã¤ä½ã‚¹ã‚³ã‚¢)\")\n",
        "\n",
        "    # 4. æ¸›ç‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
        "    if 'deductions' in summary_df.columns:\n",
        "        total_deductions = summary_df['deductions'].sum()\n",
        "        if total_deductions > 0:\n",
        "            print(\"\\nã€æ¸›ç‚¹åˆ†æã€‘\")\n",
        "            no_deduction = (summary_df['deductions'] == 0).sum()\n",
        "            with_deduction = (summary_df['deductions'] > 0).sum()\n",
        "            print(f\"  æ¸›ç‚¹ãªã—: {no_deduction} ({no_deduction/len(summary_df)*100:.1f}%)\")\n",
        "            print(f\"  æ¸›ç‚¹ã‚ã‚Š: {with_deduction} ({with_deduction/len(summary_df)*100:.1f}%)\")\n",
        "\n",
        "            # æ¸›ç‚¹ã®å½±éŸ¿åº¦\n",
        "            if with_deduction > 0:\n",
        "                impact = (summary_df[summary_df['deductions'] == 0]['final_score'].mean() -\n",
        "                         summary_df[summary_df['deductions'] > 0]['final_score'].mean())\n",
        "                print(f\"  æ¸›ç‚¹ã«ã‚ˆã‚‹å¹³å‡ã‚¹ã‚³ã‚¢ä½ä¸‹: -{impact:.2f}\")\n",
        "\n",
        "    # 5. å¤–ã‚Œå€¤ã¨ãƒªã‚¹ã‚¯ã‚¿ã‚¹ã‚¯\n",
        "    print(\"\\nã€è¦æ³¨æ„ã‚¿ã‚¹ã‚¯ã€‘\")\n",
        "    threshold = mean_score - 2 * std_score\n",
        "    outliers = summary_df[summary_df['final_score'] < threshold]\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"  å¤–ã‚Œå€¤ã‚¿ã‚¹ã‚¯ï¼ˆ< {threshold:.2f}ï¼‰: {len(outliers)}ä»¶\")\n",
        "        for _, row in outliers.head(3).iterrows():\n",
        "            print(f\"    - {row['task_id']}: {row['final_score']:.2f}\")\n",
        "    else:\n",
        "        print(\"  å¤–ã‚Œå€¤ãªã—ï¼ˆå®‰å®šã—ãŸæ€§èƒ½ï¼‰\")\n",
        "\n",
        "    # å¤±æ•—ã‚¿ã‚¹ã‚¯ï¼ˆã‚¹ã‚³ã‚¢ < 3.0ï¼‰\n",
        "    failed = summary_df[summary_df['final_score'] < 3.0]\n",
        "    if len(failed) > 0:\n",
        "        print(f\"  å¤±æ•—ã‚¿ã‚¹ã‚¯ï¼ˆ< 3.0ï¼‰: {len(failed)}ä»¶\")\n",
        "\n",
        "    # 6. æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«\n",
        "    print(\"\\nã€æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã€‘\")\n",
        "    perfect_gap = 5.0 - mean_score\n",
        "    achievable_target = summary_df['final_score'].quantile(0.75)\n",
        "    improvement_potential = achievable_target - mean_score\n",
        "\n",
        "    print(f\"  ç†è«–çš„æ”¹å–„ä½™åœ°: {perfect_gap:.2f}ãƒã‚¤ãƒ³ãƒˆ\")\n",
        "    print(f\"  ç¾å®Ÿçš„æ”¹å–„ç›®æ¨™: {achievable_target:.2f} (+{improvement_potential:.2f})\")\n",
        "\n",
        "    if available:\n",
        "        # æœ€ã‚‚æ”¹å–„åŠ¹æœãŒé«˜ã„é …ç›®\n",
        "        impacts = []\n",
        "        for c in available:\n",
        "            potential = (5.0 - summary_df[c].mean()) * (1.0 / len(available))\n",
        "            impacts.append((c, potential))\n",
        "        best_impact = max(impacts, key=lambda x: x[1])\n",
        "        print(f\"  æœ€å¤§æ”¹å–„åŠ¹æœ: {best_impact[0]} (+{best_impact[1]:.2f}ãƒã‚¤ãƒ³ãƒˆæœŸå¾…)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 7. çµæœã®ä¿å­˜\n",
        "    try:\n",
        "        # ä¿å­˜å…ˆã®æ±ºå®š\n",
        "        if 'pipeline' in locals() and hasattr(pipeline, 'run_manager'):\n",
        "            base_path = pipeline.run_manager.run_dir\n",
        "            run_id = pipeline.run_manager.run_id\n",
        "        else:\n",
        "            base_path = Path(\"./outputs/baseline\")\n",
        "            base_path.mkdir(parents=True, exist_ok=True)\n",
        "            run_id = \"baseline\"\n",
        "\n",
        "        # è©³ç´°CSV\n",
        "        csv_file = base_path / \"evaluation_results.csv\"\n",
        "        summary_df.to_csv(csv_file, index=False)\n",
        "\n",
        "        # åˆ†æã‚µãƒãƒªJSON\n",
        "        analysis = {\n",
        "            'run_id': run_id,\n",
        "            'timestamp': str(datetime.now()),\n",
        "            'performance': {\n",
        "                'mean': float(mean_score),\n",
        "                'std': float(std_score),\n",
        "                'ci_95': float(ci_95),\n",
        "                'median': float(summary_df['final_score'].median()),\n",
        "                'q1': float(summary_df['final_score'].quantile(0.25)),\n",
        "                'q3': float(summary_df['final_score'].quantile(0.75))\n",
        "            },\n",
        "            'criteria': {c: {\n",
        "                'mean': float(summary_df[c].mean()),\n",
        "                'std': float(summary_df[c].std())\n",
        "            } for c in available},\n",
        "            'summary': {\n",
        "                'num_tasks': len(summary_df),\n",
        "                'num_failed': len(failed),\n",
        "                'num_outliers': len(outliers),\n",
        "                'improvement_potential': float(improvement_potential)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        json_file = base_path / \"analysis_summary.json\"\n",
        "        with open(json_file, 'w') as f:\n",
        "            json.dump(analysis, f, indent=2)\n",
        "\n",
        "        print(f\"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
        "        print(f\"  - {csv_file}\")\n",
        "        print(f\"  - {json_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "\n",
        "    # Colabç”¨ã®è©³ç´°è¡¨ç¤º\n",
        "    if IN_COLAB:\n",
        "        from IPython.display import display\n",
        "        print(\"\\nã€ã‚¹ã‚³ã‚¢è¦ç´„çµ±è¨ˆã€‘\")\n",
        "        display(summary_df['final_score'].describe())\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Cell 11ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N5veQyUuskRD",
        "outputId": "f382b8f1-5b2c-455d-a6f1-84d01925e2b7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“Š LA-Bench 2025 è©•ä¾¡çµæœåˆ†æ\n",
            "============================================================\n",
            "\n",
            "ã€å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘\n",
            "  æœ€çµ‚ã‚¹ã‚³ã‚¢: 2.17 Â± 0.33 (95% CI)\n",
            "  ä¸­å¤®å€¤: 2.17\n",
            "  å››åˆ†ä½ç¯„å›²: [2.08, 2.25]\n",
            "  åˆ¤å®š: âš ï¸ è¦æ”¹å–„\n",
            "\n",
            "ã€ã‚¹ã‚³ã‚¢åˆ†å¸ƒã€‘\n",
            "  [1]:   0 (  0.0%) \n",
            "  [2]:   2 (100.0%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  [3]:   0 (  0.0%) \n",
            "  [4]:   0 (  0.0%) \n",
            "  [5]:   0 (  0.0%) \n",
            "\n",
            "ã€è©•ä¾¡é …ç›®åˆ†æã€‘\n",
            "  å¼·ã¿ï¼ˆä¸Šä½3é …ç›®ï¼‰:\n",
            "    safety      : 4.50Â±0.71 â—†â—†â—†â—†\n",
            "    completeness: 4.00Â±0.00 â—†â—†â—†â—†\n",
            "    feasibility : 4.00Â±0.00 â—†â—†â—†â—†\n",
            "  å¼±ã¿ï¼ˆä¸‹ä½3é …ç›®ï¼‰:\n",
            "    specificity : 3.50Â±0.71 â—‡\n",
            "    consistency : 3.00Â±0.00 â—‡â—‡\n",
            "    clarity     : 3.00Â±0.00 â—‡â—‡\n",
            "  æœ€å„ªå…ˆæ”¹å–„é …ç›®: specificity (ä¸å®‰å®šã‹ã¤ä½ã‚¹ã‚³ã‚¢)\n",
            "\n",
            "ã€æ¸›ç‚¹åˆ†æã€‘\n",
            "  æ¸›ç‚¹ãªã—: 0 (0.0%)\n",
            "  æ¸›ç‚¹ã‚ã‚Š: 2 (100.0%)\n",
            "  æ¸›ç‚¹ã«ã‚ˆã‚‹å¹³å‡ã‚¹ã‚³ã‚¢ä½ä¸‹: -nan\n",
            "\n",
            "ã€è¦æ³¨æ„ã‚¿ã‚¹ã‚¯ã€‘\n",
            "  å¤–ã‚Œå€¤ãªã—ï¼ˆå®‰å®šã—ãŸæ€§èƒ½ï¼‰\n",
            "  å¤±æ•—ã‚¿ã‚¹ã‚¯ï¼ˆ< 3.0ï¼‰: 2ä»¶\n",
            "\n",
            "ã€æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã€‘\n",
            "  ç†è«–çš„æ”¹å–„ä½™åœ°: 2.83ãƒã‚¤ãƒ³ãƒˆ\n",
            "  ç¾å®Ÿçš„æ”¹å–„ç›®æ¨™: 2.25 (+0.08)\n",
            "  æœ€å¤§æ”¹å–„åŠ¹æœ: consistency (+0.33ãƒã‚¤ãƒ³ãƒˆæœŸå¾…)\n",
            "\n",
            "============================================================\n",
            "ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:\n",
            "  - outputs/runs/20250810_093739_1729/evaluation_results.csv\n",
            "  - outputs/runs/20250810_093739_1729/analysis_summary.json\n",
            "\n",
            "ã€ã‚¹ã‚³ã‚¢è¦ç´„çµ±è¨ˆã€‘\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    2.000000\n",
              "mean     2.166667\n",
              "std      0.235702\n",
              "min      2.000000\n",
              "25%      2.083333\n",
              "50%      2.166667\n",
              "75%      2.250000\n",
              "max      2.333333\n",
              "Name: final_score, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.235702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: è©³ç´°ãªè©•ä¾¡çµæœã®è¡¨ç¤º\n",
        "#@title 13. è©³ç´°ãªè©•ä¾¡çµæœã®è¡¨ç¤º { display-mode: \"form\" }\n",
        "\n",
        "task_index = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "if 'results' in locals() and results and task_index < len(results):\n",
        "    result = results[task_index]\n",
        "    eval_result = result['evaluation']\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ“‹ Task {result['task_id']}: è©³ç´°è©•ä¾¡çµæœ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nğŸ“ å®Ÿé¨“æŒ‡ç¤º:\")\n",
        "    print(f\"  {result['instruction']}\")\n",
        "\n",
        "    print(f\"\\nğŸ“Š ç·åˆè©•ä¾¡:\")\n",
        "    print(f\"  æœ€çµ‚ã‚¹ã‚³ã‚¢: {eval_result.final_score:.2f}/5.0\")\n",
        "    print(f\"  ï¼ˆæ¸›ç‚¹å‰: {eval_result.raw_score:.2f}/5.0ï¼‰\")\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ è©•ä¾¡é …ç›®åˆ¥ã‚¹ã‚³ã‚¢:\")\n",
        "    for criterion in ['completeness', 'specificity', 'consistency',\n",
        "                     'safety', 'feasibility', 'clarity']:\n",
        "        score_obj = getattr(eval_result, criterion)\n",
        "        print(f\"  {criterion:12s}: {score_obj.score}/5 - {score_obj.reasoning[:50]}...\")\n",
        "\n",
        "    if eval_result.deductions:\n",
        "        print(f\"\\nâš ï¸ æ¸›ç‚¹é …ç›®:\")\n",
        "        for ded in eval_result.deductions:\n",
        "            print(f\"  - {ded.reason} (-{ded.points}ç‚¹)\")\n",
        "            if ded.examples:\n",
        "                print(f\"    ä¾‹: {ded.examples[0][:50]}...\")\n",
        "\n",
        "    print(f\"\\nğŸ’¬ ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯:\")\n",
        "    print(f\"  {eval_result.feedback}\")\n",
        "\n",
        "    if eval_result.improvements:\n",
        "        print(f\"\\nğŸ’¡ æ”¹å–„ææ¡ˆ:\")\n",
        "        for imp in eval_result.improvements[:3]:  # æœ€åˆã®3ã¤\n",
        "            print(f\"  - {imp}\")\n",
        "\n",
        "    print(f\"\\nğŸ”¬ ç”Ÿæˆã•ã‚ŒãŸæ‰‹é †ï¼ˆ{len(result['generated_steps'])}ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
        "    for step in result['generated_steps'][:5]:  # æœ€åˆã®5ã‚¹ãƒ†ãƒƒãƒ—\n",
        "        print(f\"  {step['id']}. {step['text'][:80]}...\")\n",
        "    if len(result['generated_steps']) > 5:\n",
        "        print(f\"  ... ä»– {len(result['generated_steps']) - 5} ã‚¹ãƒ†ãƒƒãƒ—\")\n",
        "else:\n",
        "    print(\"âš ï¸ è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23AuAcYoRhV7",
        "outputId": "8ad12c48-6d43-425d-9a66-e6be2b89310e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“‹ Task 0002: è©³ç´°è©•ä¾¡çµæœ\n",
            "============================================================\n",
            "\n",
            "ğŸ“ å®Ÿé¨“æŒ‡ç¤º:\n",
            "  24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆã§ç´°èƒã®åŸ¹åœ°äº¤æ›ã‚’è¡Œã£ã¦ãã ã•ã„\n",
            "\n",
            "ğŸ“Š ç·åˆè©•ä¾¡:\n",
            "  æœ€çµ‚ã‚¹ã‚³ã‚¢: 2.33/5.0\n",
            "  ï¼ˆæ¸›ç‚¹å‰: 3.83/5.0ï¼‰\n",
            "\n",
            "ğŸ“ˆ è©•ä¾¡é …ç›®åˆ¥ã‚¹ã‚³ã‚¢:\n",
            "  completeness: 4/5 - PPEã€BSCã€æ»…èŒãƒ”ãƒšãƒƒãƒˆã€ã‚¢ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚¿ãƒ¼ã€æ¸©ã‚ãŸåŸ¹åœ°ã€å»ƒæ£„æ‰‹é †ã€ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼æˆ»ã—ã¾ã§ä¸€é€£ã®è¦...\n",
            "  specificity : 4/5 - æ¸©åº¦ï¼ˆ37Â°Cï¼‰ã€æ¸©ã‚æ™‚é–“ï¼ˆ15â€“30åˆ†ï¼‰ã€æ·»åŠ é‡ï¼ˆ1.0 mL/ã‚¦ã‚§ãƒ«ï¼‰ã€æºã™å›æ•°ï¼ˆ3â€“5å›ï¼‰ãªã©...\n",
            "  consistency : 3/5 - æ‰‹é †å…¨ä½“ã¯è«–ç†çš„ã ãŒã€æ–‡ä¸­ã«ç¹°ã‚Šè¿”ã—ï¼ˆä¾‹ï¼š\"DMEMDMEM\"ã€\"24ã‚¦ã‚§ãƒ«24ã‚¦ã‚§ãƒ«\"ï¼‰ãŒã‚ã‚Šèª­ã¿...\n",
            "  safety      : 5/5 - PPEç€ç”¨ã€BSCå†…ä½œæ¥­ã€å»ƒæ¶²ã®10%æ¬¡äºœå¡©ç´ é…¸ãƒŠãƒˆãƒªã‚¦ãƒ ã§ã®å‡¦ç†ã€ä½œæ¥­é¢ã®ã‚¨ã‚¿ãƒãƒ¼ãƒ«æ‹­ãã€æ±šæŸ“ç‰©ã®...\n",
            "  feasibility : 4/5 - ä¸€èˆ¬çš„ãªç´°èƒåŸ¹é¤Šãƒ©ãƒœã§ãã®ã¾ã¾å®Ÿè¡Œå¯èƒ½ãªæ‰‹é †ã§ã‚ã‚‹ã€‚ãŸã ã—ã€å¸å¼•ã‚„ãƒ”ãƒšãƒƒãƒ†ã‚£ãƒ³ã‚°æ™‚ã®ç´°ã‹ãªå–ã‚Šæ‰±ã„æŒ‡...\n",
            "  clarity     : 3/5 - å¤§éƒ¨åˆ†ã¯ã‚ã‹ã‚Šã‚„ã™ã„ãŒã€èªå¥ã®é‡è¤‡ï¼ˆDMEMDMEM ç­‰ï¼‰ã‚„ã„ãã¤ã‹ã®æ›–æ˜§ãªè¡¨ç¾ï¼ˆã€Œé€Ÿã‚„ã‹ã«ã€ã€Œå¿…è¦...\n",
            "\n",
            "âš ï¸ æ¸›ç‚¹é …ç›®:\n",
            "  - ä¸è‡ªç„¶ãªæ—¥æœ¬èªï¼ˆèªå¥ã®é‡è¤‡ãƒ»ç¹°ã‚Šè¿”ã—ï¼‰: ä¾‹ï¼‰\"DMEMDMEM\"ã€\"24ã‚¦ã‚§ãƒ«24ã‚¦ã‚§ãƒ«\" ã¨ã„ã£ãŸé‡è¤‡è¡¨ç¾ã€‚ (-1.0ç‚¹)\n",
            "  - æ›–æ˜§ãªè¡¨ç¾ï¼ˆæ‰‹é †å†…ã®çŸ›ç›¾ï¼‰: ä¾‹ï¼‰ã‚¹ãƒ†ãƒƒãƒ—7ã®ã€Œå®Œå…¨ã«é™¤å»ã™ã‚‹ï¼ˆæ®‹é‡ã¯æ®‹ã•ãªã„ãŒã€ã‚¦ã‚§ãƒ«åº•é¢ã«è§¦ã‚Œãªã„ï¼‰ã€ã¯å®Ÿå‹™ä¸Šã®æ„å‘³ãŒæ›–æ˜§ã€‚ (-0.5ç‚¹)\n",
            "\n",
            "ğŸ’¬ ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯:\n",
            "  å…¨ä½“ã¨ã—ã¦åŸ¹åœ°äº¤æ›ã®æ¨™æº–æ‰‹é †ã‚’ã‚ˆãã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€å®‰å…¨é¢ã®é…æ…®ã‚‚ååˆ†ã§å®Ÿç”¨çš„ã§ã™ã€‚ãŸã ã—æ–‡æ›¸ã«ç¹°ã‚Šè¿”ã—ã‚„çŸ›ç›¾ãŒæ•£è¦‹ã•ã‚Œã€åˆå¿ƒè€…ãŒãã®ã¾ã¾èª­ã‚€ã¨å®Ÿæ–½æ™‚ã«è¿·ã†ç®‡æ‰€ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«â€œå®Œå…¨ã«é™¤å»ã™ã‚‹â€ã¨ã„ã†è¡¨ç¾ã®è§£é‡ˆï¼ˆæ®‹é‡ã‚’æ®‹ã™ã®ã‹å¦ã‹ï¼‰ã‚„ã€å¸å¼•ä½ç½®ãƒ»é€Ÿåº¦ãªã©ã®å®šé‡çš„æŒ‡ç¤ºã®æ¬ å¦‚ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚ã¾ãŸæ—¥æœ¬èªã®é‡è¤‡ã‚’ä¿®æ­£ã™ã‚‹ã¨èª­ã¿ã‚„ã™ããªã‚Šã¾ã™ã€‚\n",
            "\n",
            "ğŸ’¡ æ”¹å–„ææ¡ˆ:\n",
            "  - æ–‡æ›¸å†…ã®é‡è¤‡è¡¨ç¾ï¼ˆä¾‹ï¼š\"DMEMDMEM\"ã€\"24ã‚¦ã‚§ãƒ«24ã‚¦ã‚§ãƒ«\"ï¼‰ã‚’å‰Šé™¤ã—ã¦è‡ªç„¶ãªæ—¥æœ¬èªã«çµ±ä¸€ã™ã‚‹ã€‚\n",
            "  - ã‚¹ãƒ†ãƒƒãƒ—7ã®è¨˜è¿°ã‚’æ˜ç¢ºåŒ–ã™ã‚‹ã€‚ä¾‹ï¼šã€Œã‚¦ã‚§ãƒ«åº•é¢ã«è§¦ã‚Œãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¤ã¤ã€ä¸Šå±¤ã®åŸ¹åœ°ã‚’ã»ã¼å®Œå…¨ã«é™¤å»ã™ã‚‹ï¼ˆç›®å®‰ï¼šæ®‹é‡ â‰¤ ~50 ÂµLï¼‰ã€ã®ã‚ˆã†ã«å…·ä½“çš„é‡ã‚’ç¤ºã™ã€‚\n",
            "  - å¸å¼•ã®å…·ä½“çš„ãªå–ã‚Šæ‰±ã„ã‚’è¿½åŠ ã™ã‚‹ï¼ˆå…ˆç«¯ã‚’ã‚¦ã‚§ãƒ«å´å£ã«æ²¿ã£ã¦æ–œã‚ã«å½“ã¦ã‚‹ã€å…ˆç«¯ã¨ç´°èƒå±¤ã®è·é›¢ã®ç›®å®‰ã€å¸å¼•é€Ÿåº¦ã®æŒ‡ç¤ºãªã©ï¼‰ã€‚\n",
            "\n",
            "ğŸ”¬ ç”Ÿæˆã•ã‚ŒãŸæ‰‹é †ï¼ˆ14ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\n",
            "  1. æº–å‚™ï¼šå®Ÿé¨“ç€ãƒ»æ‰‹è¢‹ãƒ»ä¿è­·çœ¼é¡ã‚’ç€ç”¨ã™ã‚‹ã€‚ä½œæ¥­ã¯ç”Ÿç‰©å®‰å…¨ã‚­ãƒ£ãƒ“ãƒãƒƒãƒˆï¼ˆBSCã€ã‚¯ãƒªãƒ¼ãƒ³ãƒ™ãƒ³ãƒï¼‰å†…ã§è¡Œã†ã€‚å»ƒæ¶²ç”¨ã«10%æ¬¡äºœå¡©ç´ é…¸ãƒŠãƒˆãƒªã‚¦ãƒ ã‚’å«ã‚€å»ƒæ¶²å®¹å™¨ã‚’ç”¨æ„ã—ã€...\n",
            "  2. æ©Ÿå™¨ã®é›»æºãƒ»æ¸©åº¦è¨­å®šï¼šæ’æ¸©æ§½ï¼ˆæ°´æµ´ï¼‰ã‚’37Â°Cã«è¨­å®šã—ã¦é›»æºã‚’å…¥ã‚Œã€å°‘ãªãã¨ã‚‚15åˆ†å‰ã‹ã‚‰æ¸©åº¦ãŒå®‰å®šã™ã‚‹ã‚ˆã†ã«æº–å‚™ã™ã‚‹ã€‚ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ï¼ˆ37Â°Cã€5% CO2...\n",
            "  3. DMEMåŸ¹åœ°ã®æº–å‚™ï¼šä½¿ç”¨ã™ã‚‹DMEMDMEMåŸ¹åœ°ï¼ˆå®Ÿé¨“ã§ç”¨ã„ã‚‹èª¿æ•´æ¸ˆã¿DMEMï¼‰ã‚’æ»…èŒãƒœãƒˆãƒ«ã®ã¾ã¾æ’æ¸©æ§½ã®37Â°Cæ°´æµ´ã«å…¥ã‚Œã€ãƒ©ãƒ™ãƒ«é¢ãŒæ¿¡ã‚Œãªã„ã‚ˆã†ã«ã—ã¦ç´„15...\n",
            "  4. å™¨å…·ãƒ»æ¶ˆè€—å“ã®æº–å‚™ï¼šBSCå†…ã«æ»…èŒæ¸ˆã¿5 mL ã‚»ãƒ­ãƒ­ã‚¸ã‚«ãƒ«ãƒ”ãƒšãƒƒãƒˆï¼ˆå¿…è¦æ•°ï¼‰ã€ãƒ”ãƒšãƒƒãƒˆã‚¨ã‚¤ãƒ‰ã€æ»…èŒP1000ãƒãƒƒãƒ—ï¼ˆäºˆå‚™ï¼‰ã€æ»…èŒãƒ”ãƒ³ã‚»ãƒƒãƒˆã€ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ï¼ˆ70%...\n",
            "  5. 24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆç¢ºèªï¼šã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã®ãƒ‰ã‚¢ã‚’æœ€å°é™ã®é–‹é–‰ã§æ‰±ã„ã€ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ç›®çš„ã®24ã‚¦ã‚§ãƒ«24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–ã‚Šå‡ºã™ã€‚å–ã‚Šå‡ºã™å‰ã«24ã‚¦ã‚§ãƒ«ãƒ—ãƒ¬...\n",
            "  ... ä»– 9 ã‚¹ãƒ†ãƒƒãƒ—\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHEkF9V4ymXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}